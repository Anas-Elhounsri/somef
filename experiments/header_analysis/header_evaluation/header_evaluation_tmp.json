{"GPRPy-README.md": {"installation": {"excerpt": "**In the following instructions, if you use Windows, use the comands `python` and `pip`. If you use Mac or Linux, use the commands `python3` and `pip3` instead.**\n\n1) Download the GPRPy software from \n   [https://github.com/NSGeophysics/GPRPy/archive/master.zip](https://github.com/NSGeophysics/GPRPy/archive/master.zip). \n   Save the file somewhere on your computer and extract the zip folder. \n   As an **alternative**, you can install git from [https://git-scm.com/](https://git-scm.com/), then run in a command prompt:\n   `git clone https://github.com/NSGeophysics/GPRPy.git`\n   The advantage of the latter is that you can easily update your software by running from the GPRPy folder in a command prompt:\n   `git pull origin master`\n\n2) Install Python 3.7 for example from [https://conda.io/miniconda.html](https://conda.io/miniconda.html)\n\n3) Once the installation finished, open a command prompt that can run Python \n   On Windows: click on Start, then enter \"Anaconda Prompt\", without the quotation marks into the \"Search programs and files\" field. On Mac or Linux, open the regular terminal.\n\n4) In the command prompt, change to the directory  where you downloaded the GPRPy files.\n   This is usually through a command like for example\n   `cd Desktop\\GPRPy`\n   if you downloaded GPRPy directly onto your desktop. Then type the following and press enter afterward:\n   `python installMigration.py`\n   Then type the following and press enter afterward:\n   `pip install .`\n   **don't forget the period \".\" at the end of the `pip install` command**\n\n\n", "correct": true}, "issues": {"excerpt": "If you have several versions of python installed, for example on a Mac or Linux system, replace, in the commands shown earlier,\n`python` with `python3`\nand\n`pip` with `pip3`\n\nIf you have any troubles getting the software running, please send me an email or open an issue on GitHub and I will help you getting it running.\n\n\n", "correct": false}, "run": {"excerpt": "After installation, you can run the script from the Anaconda Prompt (or your Python-enabled prompt) by running either\n`gprpy`\nor\n`python -m gprpy`\n\nThe first time you run GPRPy it could take a while to initialize. GPRPy will ask you if you want to run the profile [p] or WARR / CMP [c] user interface. Type\n`p`\nand then enter for profile, or\n`c`\nand then enter for CMP / WARR.\n\nYou can also directly select one by running either\n`gprpy p`\nor\n`gprpy c`\nor\n`python -m gprpy p`\nor\n`python -m gprpy c`\n\n\n", "correct": true}}, "SRN-Deblur-README.md": {"citation": {"excerpt": "[1] `Sun et al.` J. Sun, W. Cao, Z. Xu, and J. Ponce. *Learning a convolutional\nneural network for non-uniform motion blur removal.* In CVPR, pages 769\u2013777. IEEE, 2015.\n\n[2] `Nah et al.` S. Nah, T. H. Kim, and K. M. Lee. *Deep multi-scale convolutional\nneural network for dynamic scene deblurring.* pages 3883\u20133891, 2017.\n\n[3] `Whyte et al.` O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. *Nonuniform\ndeblurring for shaken images.* International Journal on Computer Vision, 98(2):168\u2013186, 2012.", "correct": true}, "installation": {"excerpt": "Clone this project to your machine. \n\n```bash\ngit clone https://github.com/jiangsutx/SRN-Deblur.git\ncd SRN-Deblur\n```\n\n", "correct": false}, "contact": {"excerpt": "We are glad to hear if you have any suggestions and questions.\n\nPlease send email to xtao@cse.cuhk.edu.hk\n\n", "correct": true}, "requirement": {"excerpt": "- Python2.7\n- Scipy\n- Scikit-image\n- numpy\n- Tensorflow 1.4 with NVIDIA GPU or CPU (cpu testing is very slow)\n\n", "correct": true}}, "sentinelsat-README.md": {"download": {"excerpt": "  api.download()\n\n  ", "correct": false}, "usage": {"excerpt": "  ", "correct": false}}, "RDN-README.md": {"description": {"excerpt": "A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Speci\ufb01cally, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.\n\n![RDB](/Figs/RDB.png)\nFigure 1. Residual dense block (RDB) architecture.\n![RDN](/Figs/RDN.png)\nFigure 2. The architecture of our proposed residual dense network (RDN).\n\n", "correct": true}, "citation": {"excerpt": "If you find the code helpful in your resarch or work, please cite the following papers.\n```\n@InProceedings{Lim_2017_CVPR_Workshops,\n  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n  month = {July},\n  year = {2017}\n}\n\n@inproceedings{zhang2018residual,\n    title={Residual Dense Network for Image Super-Resolution},\n    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},\n    booktitle={CVPR},\n    year={2018}\n}\n\n@article{zhang2018rdnir,\n    title={Residual Dense Network for Image Restoration},\n    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},\n    booktitle={arXiv},\n    year={2018}\n}\n\n```\n", "correct": true}, "installation": {"excerpt": "1. Download DIV2K training data (800 training + 100 validtion images) from [DIV2K dataset](https://data.vision.ee.ethz.ch/cvl/DIV2K/) or [SNU_CVLab](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar).\n\n2. Place all the HR images in 'Prepare_TrainData/DIV2K/DIV2K_HR'.\n\n3. Run 'Prepare_TrainData_HR_LR_BI/BD/DN.m' in matlab to generate LR images for BI, BD, and DN models respectively.\n\n4. Run 'th png_to_t7.lua' to convert each .png image to .t7 file in new folder 'DIV2K_decoded'.\n\n5. Specify the path of 'DIV2K_decoded' to '-datadir' in 'RDN_TrainCode/code/opts.lua'.\n\nFor more informaiton, please refer to [EDSR(Torch)](https://github.com/LimBee/NTIRE2017).\n\n", "correct": true}, "usage": {"excerpt": "1. (optional) Download models for our paper and place them in '/RDN_TrainCode/experiment/model'.\n\n    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).\n\n2. Cd to 'RDN_TrainCode/code', run the following scripts to train models.\n\n    **You can use scripts in file 'TrainRDN_scripts' to train models for our paper.**\n\n    ```bash\n    #: BI, scale 2, 3, 4\n    #: BIX2F64D18C6G64P48, input=48x48, output=96x96\n    th main.lua -scale 2 -netType RDN -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true\n\n    #: BIX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX2.t7\n    th main.lua -scale 3 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true  -preTrained ../experiment/model/RDN_BIX2.t7\n\n    #: BIX4F64D18C6G64P32, input=32x32, output=128x128, fine-tune on RDN_BIX2.t7\n    th main.lua -scale 4 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 128 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true -nEpochs 1000 -preTrained ../experiment/model/RDN_BIX2.t7 \n\n    #: BD, scale 3\n    #: BDX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7\n    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BD -splitBatch 4 -trainOnly true -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7\n\n    #: DN, scale 3\n    #: DNX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7\n    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel DN -splitBatch 4 -trainOnly true  -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7\n    ```\n    Only RDN_BIX2.t7 was trained using 48x48 input patches. All other models were trained using 32x32 input patches in order to save training time.\n    However, smaller input patch size in training would lower the performance to some degree. We also set '-trainOnly true' to save GPU memory.\n", "correct": true}}, "Fiona-README.md": {"citation": {"excerpt": "        ", "correct": false}, "run": {"excerpt": "Reading Multilayer data\n-----------------------\n\nCollections can also be made from single layers within multilayer files or\ndirectories of data. The target layer is specified by name or by its integer\nindex within the file or directory. The ``fiona.listlayers()`` function\nprovides an index ordered list of layer names.\n\n.. code-block:: python\n\n    for layername in fiona.listlayers('tests/data'):\n        with fiona.open('tests/data', layer=layername) as src:\n            print(layername, len(src))\n\n    ", "correct": false}, "usage": {"excerpt": "        ", "correct": false}}, "hmr-README.md": {"citation": {"excerpt": "If you use this code for your research, please consider citing:\n```\n@inProceedings{kanazawaHMR18,\n  title={End-to-end Recovery of Human Shape and Pose},\n  author = {Angjoo Kanazawa\n  and Michael J. Black\n  and David W. Jacobs\n  and Jitendra Malik},\n  booktitle={Computer Vision and Pattern Regognition (CVPR)},\n  year={2018}\n}\n```\n\n", "correct": true}, "installation": {"excerpt": "This is only partialy tested.\n```\nconda env create -f hmr.yml\n```\n", "correct": false}, "requirement": {"excerpt": "- Python 2.7\n- [TensorFlow](https://www.tensorflow.org/) tested on version 1.3, demo alone runs with TF 1.12\n\n", "correct": true}, "usage": {"excerpt": "https://github.com/mattloper/chumpy/tree/db6eaf8c93eb5ae571eb054575fb6ecec62fd86d\n\n\n", "correct": false}}, "apsg-README.md": {"installation": {"excerpt": "Installing `apsg` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:\n\n```\nconda config --add channels conda-forge\n```\n\nOnce the `conda-forge` channel has been enabled, `apsg` can be installed with:\n\n```\nconda install apsg\n```\n\nIt is possible to list all of the versions of `apsg` available on your platform with:\n\n```\nconda search apsg --channel conda-forge\n```\n\n", "correct": true}, "documentation": {"excerpt": "Explore the full features of APSG. You can find detailed documentation [here](https://apsg.readthedocs.org).\n\n", "correct": true}, "license": {"excerpt": "APSG is free software: you can redistribute it and/or modify it under the terms of the MIT License. A copy of this license is provided in ``LICENSE`` file.\n", "correct": true}, "usage": {"excerpt": "You can see APSG in action in accompanied Jupyter notebook [http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb)\n\nAnd for fun check how simply you can animate stereonets\n[http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb)\n\n", "correct": true}}, "mplleaflet-README.md": {"installation": {"excerpt": "Install `mplleaflet` from PyPI using `$ pip install mplleaflet`.\n\n", "correct": true}, "requirement": {"excerpt": "* [jinja2](http://jinja.pocoo.org/)\n\nOptional\n* [pyproj](https://code.google.com/p/pyproj/) Only needed if you only use non-WGS-84 projections.\n* [GeoPandas](https://github.com/kjordahl/geopandas) To make your life easier.\n", "correct": true}, "usage": {"excerpt": "The simplest use is to just create your plot using matplotlib commands and call `mplleaflet.show()`.\n\n```\n>>> import matplotlib.pyplot as plt\n... #: Load longitude, latitude data\n>>> plt.hold(True)\n#: Plot the data as a blue line with red squares on top\n#: Just plot longitude vs. latitude\n>>> plt.plot(longitude, latitude, 'b') #: Draw blue line\n>>> plt.plot(longitude, latitude, 'rs') #: Draw red squares\n```\n![matplotlib x,y plot](examples/images/simple_plot.png)\n\nNormally, displaying data as longitude, latitude will cause a cartographer to cry. That's totally fine with mplleaflet, Leaflet will project your data properly.\n\n```\n#: Convert to interactive Leaflet map\n>>> import mplleaflet\n>>> mplleaflet.show()\n```\n\n[Click to view final web page](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/readme_example.html)\n\n![Leaflet map preview](examples/images/simple_plot_map_preview.jpg)\n\nDisclaimer: Displaying data in spherical mercator might also cause a cartographer to cry.\n\n`show()` allows you to specify different tile layer URLs, CRS/EPSG codes, output files, etc. \n\n", "correct": true}}, "pylops-README.md": {"installation": {"excerpt": "To ensure that further development of PyLops is performed within the same environment (i.e., same dependencies) as\nthat defined by ``requirements-dev.txt`` or ``environment-dev.yml`` files, we suggest to work off a new Conda enviroment.\n\nThe first time you clone the repository run the following command:\n```\nmake dev-install_conda\n```\nTo ensure that everything has been setup correctly, run tests:\n```\nmake tests\n```\nMake sure no tests fail, this guarantees that the installation has been successfull.\n\nRemember to always activate the conda environment every time you open a new terminal by typing:\n```\nsource activate pylops\n```\n\n", "correct": true}, "contributor": {"excerpt": "* Matteo Ravasi, mrava87\n* Carlos da Costa, cako\n* Dieter Werthm\u00fcller, prisae\n* Tristan van Leeuwen, TristanvanLeeuwen\n", "correct": true}, "documentation": {"excerpt": "The official documentation of PyLops is available [here](https://pylops.readthedocs.io/).\n\nVisit this page to get started learning about different operators and their applications as well as how to\ncreate new operators yourself and make it to the ``Contributors`` list.\n\nMoreover, if you have installed PyLops using the *developer environment* you can also build the documentation locally by\ntyping the following command:\n```\nmake doc\n```\nOnce the documentation is created, you can make any change to the source code and rebuild the documentation by\nsimply typing\n```\nmake docupdate\n```\nNote that if a new example or tutorial is created (and if any change is made to a previously available example or tutorial)\nyou are required to rebuild the entire documentation before your changes will be visible.\n\n\n", "correct": true}, "usage": {"excerpt": "You need **Python 3.6.4 or greater**.\n\n", "correct": false}}, "tensorflow-magenta-README.md": {"installation": {"excerpt": "If you have a GPU installed and you want Magenta to use it, you will need to\nfollow the [Manual Install](#manual-install) instructions, but with a few\nmodifications.\n\nFirst, make sure your system meets the [requirements to run tensorflow with GPU support](\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support).\n\nNext, follow the [Manual Install](#manual-install) instructions, but install the\n`magenta-gpu` package instead of the `magenta` package:\n\n```bash\npip install magenta-gpu\n```\n\nThe only difference between the two packages is that `magenta-gpu` depends on\n`tensorflow-gpu` instead of `tensorflow`.\n\nMagenta should now have access to your GPU.\n\n", "correct": true}, "usage": {"excerpt": "* [Installation](#installation)\n* [Using Magenta](#using-magenta)\n* [Playing a MIDI Instrument](#playing-a-midi-instrument)\n* [Development Environment (Advanced)](#development-environment)\n\n", "correct": false}}, "facebookresearch-wav2letter-README.md": {"citation": {"excerpt": "If you use the code in your paper, then please cite it as:\n\n```\n@article{pratap2018w2l,\n  author          = {Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, Ronan Collobert},\n  title           = {wav2letter++: The Fastest Open-source Speech Recognition System},\n  journal         = {CoRR},\n  volume          = {abs/1812.07625},\n  year            = {2018},\n  url             = {https://arxiv.org/abs/1812.07625},\n}\n```\n\n", "correct": true}, "documentation": {"excerpt": "- [Data Preparation](docs/data_prep.md)\n- [Training](docs/train.md)\n- [Testing / Decoding](docs/decoder.md)\n\nTo get started with wav2letter++, checkout the [tutorials](tutorials) section.\n\nWe also provide complete recipes for WSJ, Timit and Librispeech and they can be found in [recipes](recipes) folder.\n\n", "correct": true}, "license": {"excerpt": "wav2letter++ is BSD-licensed, as found in the [LICENSE](LICENSE) file.\n", "correct": true}}, "tilematrix-README.md": {}, "omfvista-README.md": {}, "PRM-README.md": {"citation": {"excerpt": "If you find the code useful for your research, please cite:\n```bibtex\n@INPROCEEDINGS{Zhou2018PRM,\n    author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},\n    title = {Weakly Supervised Instance Segmentation using Class Peak Response},\n    booktitle = {CVPR},\n    year = {2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "1. Install [Nest](https://github.com/ZhouYanzhao/Nest), a flexible tool for building and sharing deep learning modules:\n    \n    > I created Nest in the process of refactoring PRM's pytorch implementation. It aims at encouraging code reuse and ships with a bunch of useful features. PRM is now implemented as a set of Nest modules; thus you can easily install and use it as demonstrated below.\n\n    ```bash\n    $ pip install git+https://github.com/ZhouYanzhao/Nest.git\n    ```\n    \n\n2. Install PRM via Nest's CLI tool:\n\n    ```bash\n    #: note that data will be saved under your current path\n    $ nest module install github@ZhouYanzhao/PRM:pytorch prm\n    #: verify the installation\n    $ nest module list --filter prm\n    #: Output:\n    #:\n    #: 3 Nest modules found.\n    #: [0] prm.fc_resnet50 (1.0.0)\n    #: [1] prm.peak_response_mapping (1.0.0)\n    #: [2] prm.prm_visualize (1.0.0)\n    ```\n\n", "correct": true}, "requirement": {"excerpt": "* System (tested on Ubuntu 14.04LTS and Win10)\n* NVIDIA GPU + CUDA CuDNN (CPU mode is also supported but significantly slower)\n* [Python>=3.5](https://www.python.org)\n* [PyTorch>=0.4](https://pytorch.org)\n* [Jupyter Notebook](https://jupyter.org/install.html) and [ipywidgets](https://github.com/jupyter-widgets/ipywidgets) (required by the demo):\n\n    ```bash\n    #: enable the widgetsnbextension before you start the notebook server\n    jupyter nbextension enable --py --sys-prefix widgetsnbextension\n    ```\n\n", "correct": true}, "run": {"excerpt": "1. Install Nest's build-in Pytorch modules:\n\n    > To increase reusability, I abstracted some features from the original code, such as network trainer, to build Nest's built-in pytorch module set.\n    \n    ```bash\n    $ nest module install github@ZhouYanzhao/Nest:pytorch pytorch\n    ```\n\n2. Download the PASCAL-VOC2012 dataset:\n\n    ```bash\n    mkdir ./PRM/demo/datasets\n    cd ./PRM/demo/datasets\n    #: download and extract data\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n    tar xvf VOCtrainval_11-May-2012.tar\n    ```\n\n3. Run the demo experiment via [demo/main.ipynb](https://github.com/ZhouYanzhao/PRM/tree/pytorch/demo/main.ipynb)\n\n    ![PRM Segmentation](samples.png)\n\n", "correct": true}, "usage": {"excerpt": "The [pytorch branch](https://github.com/ZhouYanzhao/PRM/tree/pytorch) contains:\n\n* the **pytorch** implementation of Peak Response Mapping (Stimulation and Backprop).\n* the PASCAL-VOC demo (training, inference, and visualization).\n\nPlease follow the instruction below to install it and run the experiment demo.\n\n", "correct": true}}, "GAN_stability-README.md": {"usage": {"excerpt": "First download your data and put it into the `./data` folder.\n\nTo train a new model, first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using\n```\npython train.py PATH_TO_CONFIG\n```\n\nTo compute the inception score for your model and generate samples, use\n```\npython test.py PATH_TO_CONIFG\n```\n\nFinally, you can create nice latent space interpolations using\n```\npython interpolate.py PATH_TO_CONFIG\n```\nor\n```\npython interpolate_class.py PATH_TO_CONFIG\n```\n\n", "correct": true}}, "d3-README.md": {"installation": {"excerpt": "If you use npm, `npm install d3`. Otherwise, download the [latest release](https://github.com/d3/d3/releases/latest). The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from [d3js.org](https://d3js.org), [CDNJS](https://cdnjs.com/libraries/d3), or [unpkg](https://unpkg.com/d3/). For example:\n\n```html\n\n```\n\nFor the minified version:\n\n```html\n\n```\n\nYou can also use the standalone D3 microlibraries. For example, [d3-selection](https://github.com/d3/d3-selection):\n\n```html\n\n```\n\nD3 is written using [ES2015 modules](http://www.2ality.com/2014/09/es6-modules-final.html). Create a [custom bundle using Rollup](https://bl.ocks.org/mbostock/bb09af4c39c79cffcde4), Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:\n\n```js\nimport {scaleLinear} from \"d3-scale\";\n```\n\nOr import everything into a namespace (here, `d3`):\n\n```js\nimport * as d3 from \"d3\";\n```\n\nIn Node:\n\n```js\nvar d3 = require(\"d3\");\n```\n\nYou can also require individual modules and combine them into a `d3` object using [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign):\n\n```js\nvar d3 = Object.assign({}, require(\"d3-format\"), require(\"d3-geo\"), require(\"d3-geo-projection\"));\n```\n", "correct": true}}, "microsoft-malmo-README.md": {"citation": {"excerpt": "Please cite Malmo as:\n\nJohnson M., Hofmann K., Hutton T., Bignell D. (2016) [_The Malmo Platform for Artificial Intelligence Experimentation._](http://www.ijcai.org/Proceedings/16/Papers/643.pdf) [Proc. 25th International Joint Conference on Artificial Intelligence](http://www.ijcai.org/Proceedings/2016), Ed. Kambhampati S., p. 4246. AAAI Press, Palo Alto, California USA. https://github.com/Microsoft/malmo\n\n----\n\n", "correct": true}, "run": {"excerpt": "```\ncd Python_Examples\npython3 run_mission.py\n``` \n\n", "correct": true}}, "da-faster-rcnn-README.md": {"citation": {"excerpt": "The implementation is built on the python implementation of Faster RCNN [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)\n\n", "correct": false}, "usage": {"excerpt": "1. Build Caffe and pycaffe (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))\n\n2. Build the Cython modules\n    ```Shell\n    cd $FRCN_ROOT/lib\n    make\n    \n3. Follow the instrutions of [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn) to download related data.\n    \n4. Prepare the dataset, source domain data should start with the filename 'source_', and target domain data with 'target_'.\n\n5. To train the Domain Adaptive Faster R-CNN:\n    ```Shell\n    cd $FRCN_ROOT\n    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  {NUM_ITER}  --cfg  {CONFIGURATION_FILE}\n    \n", "correct": true}}, "facebookresearch-ResNeXt-README.md": {"description": {"excerpt": "This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).\n\n[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \u201ccardinality\u201d (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n\n\n![teaser](http://vcl.ucsd.edu/resnext/teaser.png)\n", "correct": true}, "citation": {"excerpt": "| baseWidth | cardinality |\n|---------- | ----------- |\n| 64        | 1           |\n| 40        | 2           |\n| 24        | 4           |\n| 14        | 8           |\n| 4         | 32          |\n\n\nTo train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:\n```bash\nth main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]\n```\n\nTo reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:\n```bash\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true\n```\nTo get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:\n```bash\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true\n```\nNote: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.\n\n", "correct": false}, "issues": {"excerpt": "| Network             | GFLOPS | Top-1 Error |  Download   |\n| ------------------- | ------ | ----------- | ------------|\n| ResNet-50 (1x64d)   |  ~4.1  |  23.9        | [Original ResNet-50](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)       |\n| ResNeXt-50 (32x4d)  |  ~4.1  |  22.2        | [Download (191MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_50_32x4d.t7)       |\n| ResNet-101 (1x64d)  |  ~7.8  |  22.0        | [Original ResNet-101](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)      |\n| ResNeXt-101 (32x4d) |  ~7.8  |  21.2        | [Download (338MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_32x4d.t7)      |\n| ResNeXt-101 (64x4d) |  ~15.6 |  20.4        | [Download (638MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_64x4d.t7)       |\n\n", "correct": false}, "requirement": {"excerpt": "See the fb.resnet.torch [installation instructions](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md) for a step-by-step guide.\n- Install [Torch](http://torch.ch/docs/getting-started.html) on a machine with CUDA GPU\n- Install [cuDNN v4 or v5](https://developer.nvidia.com/cudnn) and the Torch [cuDNN bindings](https://github.com/soumith/cudnn.torch/tree/R4)\n- Download the [ImageNet](http://image-net.org/download-images) dataset and [move validation images](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset) to labeled subfolders\n\n", "correct": true}}, "vue-devtools-README.md": {"installation": {"excerpt": "1. Fixing \"Download the Vue Devtools for a better development experience\" console message when working locally over `file://` protocol:\n  1.1 - Google Chrome: Right click on vue-devtools icon and click \"Manage Extensions\" then search for vue-devtools on the extensions list. Check the \"Allow access to file URLs\" box.\n\n2. How to use the devtools in IE/Edge/Safari or any other browser? [Get the standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)\n\n\n", "correct": false}, "issues": {"excerpt": "1. Fixing \"Download the Vue Devtools for a better development experience\" console message when working locally over `file://` protocol:\n  1.1 - Google Chrome: Right click on vue-devtools icon and click \"Manage Extensions\" then search for vue-devtools on the extensions list. Check the \"Allow access to file URLs\" box.\n\n2. How to use the devtools in IE/Edge/Safari or any other browser? [Get the standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)\n\n\n", "correct": false}, "license": {"excerpt": "[MIT](http://opensource.org/licenses/MIT)\n", "correct": true}, "usage": {"excerpt": "1. If the page uses a production/minified build of Vue.js, devtools inspection is disabled by default so the Vue pane won't show up.\n\n2. To make it work for pages opened via `file://` protocol, you need to check \"Allow access to file URLs\" for this extension in Chrome's extension management panel.\n\n", "correct": false}}, "tippecanoe-README.md": {"description": {"excerpt": " * `-A` *attribution* or `--attribution=`*attribution*: Set the attribution string.\n * `-n` *name* or `--name=`*name*: Set the tileset name.\n * `-N` *description* or `--description=`*description*: Set the tileset description.\n\n", "correct": false}, "installation": {"excerpt": " * `-pk` or `--no-tile-size-limit`: Don't skip tiles larger than 500K.\n * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.\n * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.\n\nBecause tile-join just copies the geometries to the new .mbtiles without processing them\n(except to rescale the extents if necessary),\nit doesn't have any of tippecanoe's recourses if the new tiles are bigger than the 500K tile limit.\nIf a tile is too big and you haven't specified `-pk`, it is just left out of the new tileset.\n\nExample\n-------\n\nImagine you have a tileset of census blocks:\n\n```sh\ncurl -O http://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_06001_tabblock10.zip\nunzip tl_2010_06001_tabblock10.zip\nogr2ogr -f GeoJSON tl_2010_06001_tabblock10.json tl_2010_06001_tabblock10.shp\n./tippecanoe -o tl_2010_06001_tabblock10.mbtiles tl_2010_06001_tabblock10.json\n```\n\nand a CSV of their populations:\n\n```sh\ncurl -O http://www2.census.gov/census_2010/01-Redistricting_File--PL_94-171/California/ca2010.pl.zip\nunzip -p ca2010.pl.zip cageo2010.pl |\nawk 'BEGIN {\n    print \"GEOID10,population\"\n}\n(substr($0, 9, 3) == \"750\") {\n    print \"\\\"\" substr($0, 28, 2) substr($0, 30, 3) substr($0, 55, 6) substr($0, 62, 4) \"\\\",\" (0 + substr($0, 328, 9))\n}' > population.csv\n```\n\nwhich looks like this:\n\n```\nGEOID10,population\n\"060014277003018\",0\n\"060014283014046\",0\n\"060014284001020\",0\n...\n\"060014507501001\",202\n\"060014507501002\",119\n\"060014507501003\",193\n\"060014507501004\",85\n...\n```\n\nThen you can join those populations to the geometries and discard the no-longer-needed ID field:\n\n```sh\n./tile-join -o population.mbtiles -x GEOID10 -c population.csv tl_2010_06001_tabblock10.mbtiles\n```\n\ntippecanoe-enumerate\n====================\n\nThe `tippecanoe-enumerate` utility lists the tiles that an `mbtiles` file defines.\nEach line of the output lists the name of the `mbtiles` file and the zoom, x, and y\ncoordinates of one of the tiles. It does basically the same thing as\n\n    select zoom_level, tile_column, (1 << zoom_level) - 1 - tile_row from tiles;\n\non the file in sqlite3.\n\ntippecanoe-decode\n=================\n\nThe `tippecanoe-decode` utility turns vector mbtiles back to GeoJSON. You can use it either\non an entire file:\n\n    tippecanoe-decode file.mbtiles\n\nor on an individual tile:\n\n    tippecanoe-decode file.mbtiles zoom x y\n    tippecanoe-decode file.vector.pbf zoom x y\n\nUnless you use `-c`, the output is a set of nested FeatureCollections identifying each\ntile and layer separately. Note that the same features generally appear at all zooms,\nso the output for the file will have many copies of the same features at different\nresolutions.\n\n", "correct": false}, "update": {"excerpt": "```\n#: Retrieve and tile California 2000 Census tracts\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2000/tl_2010_06_tract00.zip\nunzip tl_2010_06_tract00.zip\nogr2ogr -f GeoJSON tl_2010_06_tract00.shp.json tl_2010_06_tract00.shp\ntippecanoe -z11 -o tracts.mbtiles -l tracts tl_2010_06_tract00.shp.json\n\n#: Create a copy of the tileset, minus Alameda County (FIPS code 001)\ntile-join -j '{\"*\":[\"none\",[\"==\",\"COUNTYFP00\",\"001\"]]}' -f -o tracts-filtered.mbtiles tracts.mbtiles\n\n#: Retrieve and tile Alameda County Census tracts for 2010\ncurl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_06001_tract10.zip\nunzip tl_2010_06001_tract10.zip\nogr2ogr -f GeoJSON tl_2010_06001_tract10.shp.json tl_2010_06001_tract10.shp\ntippecanoe -z11 -o tracts-added.mbtiles -l tracts tl_2010_06001_tract10.shp.json\n\n#: Merge the filtered tileset and the tileset of new tracts into a final tileset\ntile-join -o tracts-final.mbtiles tracts-filtered.mbtiles tracts-added.mbtiles\n```\n\nThe `-z11` option explicitly specifies the maxzoom, to make sure both the old and new tilesets have the same zoom range.\n\nThe `-j` option to `tile-join` specifies a filter, so that only the desired features will be copied to the new tileset.\nThis filter excludes (using `none`) any features whose FIPS code (`COUNTYFP00`) is the code for Alameda County (`001`).\n\nOptions\n-------\n\nThere are a lot of options. A lot of the time you won't want to use any of them\nother than `-o` _output_`.mbtiles` to name the output file, and probably `-f` to\ndelete the file that already exists with that name.\n\nIf you aren't sure what the right maxzoom is for your data, `-zg` will guess one for you\nbased on the density of features.\n\nTippecanoe will normally drop a fraction of point features at zooms below the maxzoom,\nto keep the low-zoom tiles from getting too big. If you have a smaller data set where\nall the points would fit without dropping any of them, use `-r1` to keep them all.\nIf you do want point dropping, but you still want the tiles to be denser than `-zg`\nthinks they should be, use `-B` to set a basezoom lower than the maxzoom.\n\nIf some of your tiles are coming out too big in spite of the settings above, you will\noften want to use `--drop-densest-as-needed` to drop whatever fraction of the features\nis necessary at each zoom level to make that zoom level's tiles work.\n\nIf your features have a lot of attributes, use `-y` to keep only the ones you really need.\n\nIf your input is formatted as newline-delimited GeoJSON, use `-P` to make input parsing a lot faster.\n\n", "correct": false}, "usage": {"excerpt": "```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\nunzip ne_10m_admin_0_countries.zip\nogr2ogr -f GeoJSON ne_10m_admin_0_countries.geojson ne_10m_admin_0_countries.shp\n\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\nunzip -o ne_10m_admin_1_states_provinces.zip\nogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp\n\ntippecanoe -z3 -o countries-z3.mbtiles --coalesce-densest-as-needed ne_10m_admin_0_countries.geojson\ntippecanoe -zg -Z4 -o states-Z4.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson\ntile-join -o states-countries.mbtiles countries-z3.mbtiles states-Z4.mbtiles\n```\n\nCountries:\n\n* `-z3`: Only generate zoom levels 0 through 3\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n\nStates and Provinces:\n\n* `-Z4`: Only generate zoom levels 4 and beyond\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n", "correct": true}}, "lasio-README.md": {"documentation": {"excerpt": "See here for the [complete lasio package documentation](https://lasio.readthedocs.io/en/latest/).\n\n", "correct": true}, "license": {"excerpt": "MIT\n", "correct": true}, "usage": {"excerpt": "Install the usual way:\n\n```bash\n$ pip install lasio\n```\n\nVery quick example session:\n\n```python\n>>> import lasio\n>>> las = lasio.read(\"sample_big.las\")\n```\n\nData is accessible both directly as numpy arrays\n\n```python\n>>> las.keys()\n['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']\n>>> las['SFLU']\narray([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])\n>>> las['DEPT']\narray([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,\n        1669.875])\n```\n\nand as ``CurveItem`` objects with associated metadata:\n\n```python\n>>> las.curves\n[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)), \nCurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)), \nCurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)), \nCurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)), \nCurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)), \nCurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)), \nCurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)), \nCurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]\n```\n\nHeader information is parsed into simple HeaderItem objects, and stored in a dictionary for each section of the header:\n\n```python\n>>> las.version\n[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS), \nHeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]\n>>> las.well\n[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT), \nHeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP), \nHeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP), \nHeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL), \nHeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP), \nHeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #:12, descr=WELL, original_mnemonic=WELL), \nHeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD), \nHeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC), \nHeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV), \nHeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC), \nHeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE), \nHeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]\n>>> las.params\n[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT), \nHeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS), \nHeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD), \nHeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR), \nHeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN), \nHeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF), \nHeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]\n```\n\nThe data is stored as a 2D numpy array:\n\n```python\n>>> las.data\narray([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       ...,\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])\n```\n\nYou can also retrieve and load data as a ``pandas`` DataFrame, build LAS files from scratch, \nwrite them back to disc, and export to Excel, amongst other things.\n\nSee the [documentation](https://lasio.readthedocs.io/en/latest/) for more details.\n\n", "correct": true}}, "LibGEOS.jl-README.md": {"installation": {"excerpt": "1. At the Julia prompt, run \n  ```julia\n  julia> Pkg.add(\"LibGEOS\")\n  ```\n  This will install both the Julia package and GEOS shared libraries together. To just reinstall the GEOS shared libraries, run `Pkg.build(\"LibGEOS\")`.\n\n2. Test that `LibGEOS` works by runnning\n  ```julia\n  julia> Pkg.test(\"LibGEOS\")\n  ```\n", "correct": true}}, "DID-MDN-README.md": {"citation": {"excerpt": "Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and help from [Hang Zhang](http://hangzh.com/)\n", "correct": false}, "usage": {"excerpt": "\tpython test.py --dataroot ./facades/github --valDataroot ./facades/github --netG ./pre_trained/netG_epoch_9.pth \u00a0 \nPre-trained model can be downloaded at (put it in the folder 'pre_trained'): https://drive.google.com/drive/folders/1VRUkemynOwWH70bX9FXL4KMWa4s_PSg2?usp=sharing\n\nPre-trained density-aware model can be downloaded at (Put it in the folder 'classification'): https://drive.google.com/drive/folders/1-G86JTvv7o1iTyfB2YZAQTEHDtSlEUKk?usp=sharing\n\nPre-trained residule-aware model can be downloaded at (Put it in the folder 'residual_heavy'): https://drive.google.com/drive/folders/1bomrCJ66QVnh-WduLuGQhBC-aSWJxPmI?usp=sharing\n\n", "correct": true}}, "bootstrap-README.md": {"documentation": {"excerpt": "Bootstrap's documentation, included in this repo in the root directory, is built with [Hugo](https://gohugo.io/) and publicly hosted on GitHub Pages at . The docs may also be run locally.\n\nDocumentation search is powered by [Algolia's DocSearch](https://community.algolia.com/docsearch/). Working on our search? Be sure to set `debug: true` in `site/static/docs/4.3/assets/js/src/search.js` file.\n\n", "correct": true}, "license": {"excerpt": "Code and documentation copyright 2011-2019 the [Bootstrap Authors](https://github.com/twbs/bootstrap/graphs/contributors) and [Twitter, Inc.](https://twitter.com) Code released under the [MIT License](https://github.com/twbs/bootstrap/blob/master/LICENSE). Docs released under [Creative Commons](https://github.com/twbs/bootstrap/blob/master/docs/LICENSE).\n", "correct": true}, "run": {"excerpt": "1. Run `npm install` to install the Node.js dependencies, including Hugo (the site builder).\n2. Run `npm run test` (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.\n3. From the root `/bootstrap` directory, run `npm run docs-serve` in the command line.\n4. Open `http://localhost:9001/` in your browser, and voil\u00e0.\n\nLearn more about using Hugo by reading its [documentation](https://gohugo.io/documentation/).\n\n", "correct": true}, "usage": {"excerpt": "Several quick start options are available:\n\n- [Download the latest release.](https://github.com/twbs/bootstrap/archive/v4.3.1.zip)\n- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`\n- Install with [npm](https://www.npmjs.com/): `npm install bootstrap`\n- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@4.3.1`\n- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:4.3.1`\n- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`\n\nRead the [Getting started page](https://getbootstrap.com/docs/4.3/getting-started/introduction/) for information on the framework contents, templates and examples, and more.\n\n\n", "correct": true}}, "pyvista-README.md": {"description": {"excerpt": "* Embeddable rendering in Jupyter Notebooks\n* Filtering/plotting tools built for interactivity in Jupyter notebooks (see `IPython Tools`_)\n* Direct access to mesh analysis and transformation routines (see Filters_)\n* Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)\n* Import meshes from many common formats (use ``pyvista.read()``)\n* Export meshes as VTK, STL, OBJ, or PLY file types\n\n\n.. _IPython Tools: http://docs.pyvista.org/tools/ipy_tools.html\n.. _Filters: http://docs.pyvista.org/tools/filters.html\n.. _Plotting: http://docs.pyvista.org/tools/plotting.html\n\n\n", "correct": true}, "citation": {"excerpt": "There is a `paper about PyVista `_!\n\nIf you are using PyVista in your scientific research, please help our scientific\nvisibility by citing our work!\n\n\n    Sullivan et al., (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450\n\n\nBibTex:\n\n.. code::\n\n    @article{sullivan2019pyvista,\n      doi = {10.21105/joss.01450},\n      url = {https://doi.org/10.21105/joss.01450},\n      year = {2019},\n      month = {may},\n      publisher = {The Open Journal},\n      volume = {4},\n      number = {37},\n      pages = {1450},\n      author = {C. Bane Sullivan and Alexander Kaszynski},\n      title = {{PyVista}: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit ({VTK})},\n      journal = {Journal of Open Source Software}\n    }\n", "correct": true}}, "sequelize-sequelize-README.md": {"installation": {"excerpt": "```bash\n$ npm install --save sequelize #: This will install v5\n\n#: And one of the following:\n$ npm install --save pg pg-hstore #: Postgres\n$ npm install --save mysql2\n$ npm install --save mariadb\n$ npm install --save sqlite3\n$ npm install --save tedious #: Microsoft SQL Server\n```\n\n", "correct": true}, "documentation": {"excerpt": "- [v5 Documentation](https://sequelize.org/master)\n- [v4 Documentation](https://sequelize.org/v4)\n- [v3 Documentation](https://sequelize.org/v3)\n- [Contributing](https://github.com/sequelize/sequelize/blob/master/CONTRIBUTING.md)\n\n", "correct": true}}, "DCPDN-README.md": {"citation": {"excerpt": "Great thanks for the insight discussion with [Vishwanath Sindagi](http://www.vishwanathsindagi.com/) and initial discussion with [Dr. Kevin S. Zhou](https://sites.google.com/site/skevinzhou/home)\n\nThis work is under MIT license.\n", "correct": false}, "usage": {"excerpt": "\tpython demo.py --dataroot ./facades/nat_new4 --valDataroot ./facades/nat_new4 --netG ./demo_model/netG_epoch_8.pth \u00a0 \nPre-trained dehazing model can be downloaded at (put it in the folder 'demo_model'): https://drive.google.com/drive/folders/1BmNP5ZUWEFeGGEL1NsZSRbYPyjBQ7-nn?usp=sharing\n\nTesting images (nature)  can be downloaded at (put it in the folder 'facades'):\nhttps://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing\n\nTesting images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):\nhttps://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing\n\n\n", "correct": true}}, "Shapely-README.md": {"installation": {"excerpt": "Shapely may be installed from a source distribution or one of several kinds\nof built distribution.\n\n", "correct": false}}, "react-README.md": {"installation": {"excerpt": "React has been designed for gradual adoption from the start, and **you can use as little or as much React as you need**:\n\n* Use [Online Playgrounds](https://reactjs.org/docs/getting-started.html#online-playgrounds) to get a taste of React.\n* [Add React to a Website](https://reactjs.org/docs/add-react-to-a-website.html) as a `` tag in one minute.\n* [Create a New React App](https://reactjs.org/docs/create-a-new-react-app.html) if you're looking for a powerful JavaScript toolchain.\n\nYou can use React as a `` tag from a [CDN](https://reactjs.org/docs/cdn-links.html), or as a `react` package on [npm](https://www.npmjs.com/).\n\n", "correct": false}, "documentation": {"excerpt": "You can find the React documentation [on the website](https://reactjs.org/docs).  \n\nCheck out the [Getting Started](https://reactjs.org/docs/getting-started.html) page for a quick overview.\n\nThe documentation is divided into several sections:\n\n* [Tutorial](https://reactjs.org/tutorial/tutorial.html)\n* [Main Concepts](https://reactjs.org/docs/hello-world.html)\n* [Advanced Guides](https://reactjs.org/docs/jsx-in-depth.html)\n* [API Reference](https://reactjs.org/docs/react-api.html)\n* [Where to Get Support](https://reactjs.org/community/support.html)\n* [Contributing Guide](https://reactjs.org/docs/how-to-contribute.html)\n\nYou can improve it by sending pull requests to [this repository](https://github.com/reactjs/reactjs.org).\n\n", "correct": true}, "issues": {"excerpt": "To help you get your feet wet and get you familiar with our contribution process, we have a list of [good first issues](https://github.com/facebook/react/labels/good%20first%20issue) that contain bugs which have a relatively limited scope. This is a great place to get started.\n\n", "correct": false}, "license": {"excerpt": "React is [MIT licensed](./LICENSE).\n", "correct": true}, "usage": {"excerpt": "We have several examples [on the website](https://reactjs.org/). Here is the first one to get you started:\n\n```jsx\nfunction HelloMessage({ name }) {\n  return Hello {name};\n}\n\nReactDOM.render(\n  ,\n  document.getElementById('container')\n);\n```\n\nThis example will render \"Hello Taylor\" into a container on the page.\n\nYou'll notice that we used an HTML-like syntax; [we call it JSX](https://reactjs.org/docs/introducing-jsx.html). JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a `` tag, read [this section](https://reactjs.org/docs/add-react-to-a-website.html#optional-try-react-with-jsx) on integrating JSX; otherwise, the [recommended JavaScript toolchains](https://reactjs.org/docs/create-a-new-react-app.html) handle it automatically.\n\n", "correct": true}}, "pose-residual-network-pytorch-README.md": {"citation": {"excerpt": "If you find this code useful for your research, please consider citing our paper:\n```\n@Inproceedings{kocabas18prn,\n  Title          = {Multi{P}ose{N}et: Fast Multi-Person Pose Estimation using Pose Residual Network},\n  Author         = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},\n  Booktitle      = {European Conference on Computer Vision (ECCV)},\n  Year           = {2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "1. Clone this repository \n`git clone https://github.com/salihkaragoz/pose-residual-network-pytorch.git`\n\n2. Install [Pytorch](https://pytorch.org/)\n\n3. `pip install -r src/requirements.txt`\n\n4. To download COCO dataset train2017 and val2017 annotations run: `bash data/coco.sh`. (data size: ~240Mb)\n\n", "correct": true}, "requirement": {"excerpt": "```\npython\npytorch\nnumpy\ntqdm\npycocotools\nprogress\nscikit-image\n```\n\n", "correct": true}, "usage": {"excerpt": "We have tested our method on [Coco Dataset](http://cocodataset.org)\n\n", "correct": false}}, "gempy-README.md": {"citation": {"excerpt": "* de la Varga, M., Schaaf, A., and Wellmann, F.: GemPy 1.0: open-source stochastic geological modeling and inversion, Geosci. Model Dev., 12, 1-32, https://doi.org/10.5194/gmd-12-1-2019, 2019\n* Calcagno, P., Chil\u00e8s, J. P., Courrioux, G., & Guillen, A. (2008). Geological modelling from field data and geological knowledge: Part I. Modelling method coupling 3D potential-field interpolation and geological rules. Physics of the Earth and Planetary Interiors, 171(1-4), 147-157.\n* Lajaunie, C., Courrioux, G., & Manuel, L. (1997). Foliation fields and 3D cartography in geology: principles of a method based on potential interpolation. Mathematical Geology, 29(4), 571-584.\n", "correct": false}, "installation": {"excerpt": "1) Install CUDA if you do not have it already.\n\n2) Install Anaconda3 2019.03 with Python 3.7 (this is the last release).\n\n3) Install Theano and associated packages from the Anaconda prompt as administrator, and finally install GemPy 2.0:\n\n- conda update --all\n- conda install libpython\n- conda install m2w64-toolchain\n- conda install git\n- conda install pygpu\n- pip install theano==1.0.4\n- pip install gempy==2.0b0.dev2\n\nNote that:\n\na) some other packages required by Theano are already included in Anaconda: numpy, scipy, mkl-service, nose, and sphinx.\n\nb) pydot-ng (suggested on Theano web site) yields a lot of errors. I dropped this. It is needed to handle large picture for gif/images and probably it is not needed by GemPy.\n\nc) Trying to install all the packages in one go but it does not work, as well as doing the same in Anaconda Navigator, or installing an older Anaconda release with Python 3.5 (Anaconda3 4.2.0) as indicated in some tutorial on Theano.\n\n\n\n", "correct": true}, "documentation": {"excerpt": "Extensive documentation for *GemPy* is hosted at [gempy.readthedocs.io](http://gempy.readthedocs.io/),\nexplaining its capabilities, [the theory behind it](http://gempy.readthedocs.io/Kriging.html) and \nproviding detailed [tutorials](http://gempy.readthedocs.io/tutorial.html) on how to use it.\n\n\n", "correct": true}, "requirement": {"excerpt": "*GemPy* requires Python 3 and makes use of numerous open-source libraries:\n\n* pandas>=0.21.0\n* cython\n* Theano\n* matplotlib\n* numpy\n* pytest\n* nbsphinx\n* seaborn\n* networkx\n* ipywidgets\n\nOptional:\n\n* git+git://github.com/Leguark/scikit-image@master\n* steno3d\n* vtk\n* gdal\n* qgrid\n* pymc\n* pymc3\n\n* `vtk>=7` for interactive 3-D visualization \n* `pymc` or `pymc3`\n* `steno3d` \n\nOverall we recommend the use of a dedicated Python distribution, such as \n[Anaconda](https://www.continuum.io/what-is-anaconda), for hassle-free package installation. \nWe are currently working on providing GemPy also via Anaconda Cloud, for easier installation of\nits dependencies.\n\n", "correct": true}}, "iter-reason-README.md": {"citation": {"excerpt": "```\n@inproceedings{chen18iterative,\n    author = {Xinlei Chen and Li-Jia Li and Li Fei-Fei and Abhinav Gupta},\n    title = {Iterative Visual Reasoning Beyond Convolutions},\n    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n    Year = {2018}\n}\n```\n\nThe idea of spatial memory was developed in:\n```\n@inproceedings{chen2017spatial,\n    author = {Xinlei Chen and Abhinav Gupta},\n    title = {Spatial Memory for Context Reasoning in Object Detection},\n    booktitle = {Proceedings of the International Conference on Computer Vision},\n    Year = {2017}\n}\n```", "correct": true}, "installation": {"excerpt": "1. Clone the repository.\n  ```Shell\n  git clone https://github.com/endernewton/iter-reason.git\n  cd iter-reason\n  ```\n\n2. Set up data, here we use [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) as an example.\n  ```Shell\n  mkdir -p data/ADE\n  cd data/ADE\n  wget -v http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip\n  tar -xzvf ADE20K_2016_07_26.zip\n  mv ADE20K_2016_07_26/* ./\n  rmdir ADE20K_2016_07_26\n  #: then get the train/val/test split\n  wget -v http://xinleic.xyz/data/ADE_split.tar.gz\n  tar -xzvf ADE_split.tar.gz\n  rm -vf ADE_split.tar.gz\n  cd ../..\n  ```\n\n3. Set up pre-trained ImageNet models. This is similarly done in [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn). Here by default we use ResNet-50 as the backbone:\n  ```Shell\n   mkdir -p data/imagenet_weights\n   cd data/imagenet_weights\n   wget -v http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n   tar -xzvf resnet_v1_50_2016_08_28.tar.gz\n   mv resnet_v1_50.ckpt res50.ckpt\n   cd ../..\n   ```\n\n4. Compile the library (for computing bounding box overlaps).\n  ```Shell\n  cd lib\n  make\n  cd ..\n  ```\n\n5. Now you are ready to run! For example, to train and test the baseline:\n  ```Shell\n  ./experiments/scripts/train.sh [GPU_ID] [DATASET] [NET] [STEPS] [ITER] \n  #: GPU_ID is the GPU you want to test on\n  #: DATASET in {ade, coco, vg} is the dataset to train/test on, defined in the script\n  #: NET in {res50, res101} is the backbone networks to choose from\n  #: STEPS (x10K) is the number of iterations before it reduces learning rate, can support multiple steps separated by character 'a'\n  #: ITER (x10K) is the total number of iterations to run\n  #: Examples:\n  #: train on ADE20K for 320K iterations, reducing learning rate at 280K.\n  ./experiments/scripts/train.sh 0 ade 28 32\n  #: train on COCO for 720K iterations, reducing at 320K and 560K.\n  ./experiments/scripts/train.sh 1 coco 32a56 72\n  ```\n\n6. To train and test the reasoning modules (based on ResNet-50):\n  ```Shell\n  ./experiments/scripts/train_memory.sh [GPU_ID] [DATASET] [MEM] [STEPS] [ITER] \n  #: MEM in {local} is the type of reasoning modules to use \n  #: Examples:\n  #: train on ADE20K on the local spatial memory.\n  ./experiments/scripts/train_memory.sh 0 ade local 28 32\n  ```\n\n7. Once the training is done, you can test the models separately with `test.sh` and `test_memory.sh`, we also provided a separate set of scripts to test on larger image inputs.\n\n8. You can use tensorboard to visualize and track the progress, for example:\n  ```Shell\n  tensorboard --logdir=tensorboard/res50/ade_train_5/ --port=7002 &\n  ```\n\n", "correct": true}, "requirement": {"excerpt": "1. Tensorflow, tested with version 1.6 with Ubuntu 16.04, installed with:\n  ```Shell\n  pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl\n  ```\n\n2. Other packages needed can be installed with `pip`:\n  ```Shell\n  pip install Cython easydict matplotlib opencv-python Pillow pyyaml scipy\n  ```\n\n3. For running COCO, the API can be installed globally:\n  ```Shell\n  #: any path is okay\n  mkdir ~/install && cd ~/install\n  git clone https://github.com/cocodataset/cocoapi.git cocoapi\n  cd cocoapi/PythonAPI\n  python setup.py install --user\n  ```\n\n", "correct": true}, "run": {"excerpt": "1. Clone the repository.\n  ```Shell\n  git clone https://github.com/endernewton/iter-reason.git\n  cd iter-reason\n  ```\n\n2. Set up data, here we use [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) as an example.\n  ```Shell\n  mkdir -p data/ADE\n  cd data/ADE\n  wget -v http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip\n  tar -xzvf ADE20K_2016_07_26.zip\n  mv ADE20K_2016_07_26/* ./\n  rmdir ADE20K_2016_07_26\n  #: then get the train/val/test split\n  wget -v http://xinleic.xyz/data/ADE_split.tar.gz\n  tar -xzvf ADE_split.tar.gz\n  rm -vf ADE_split.tar.gz\n  cd ../..\n  ```\n\n3. Set up pre-trained ImageNet models. This is similarly done in [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn). Here by default we use ResNet-50 as the backbone:\n  ```Shell\n   mkdir -p data/imagenet_weights\n   cd data/imagenet_weights\n   wget -v http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n   tar -xzvf resnet_v1_50_2016_08_28.tar.gz\n   mv resnet_v1_50.ckpt res50.ckpt\n   cd ../..\n   ```\n\n4. Compile the library (for computing bounding box overlaps).\n  ```Shell\n  cd lib\n  make\n  cd ..\n  ```\n\n5. Now you are ready to run! For example, to train and test the baseline:\n  ```Shell\n  ./experiments/scripts/train.sh [GPU_ID] [DATASET] [NET] [STEPS] [ITER] \n  #: GPU_ID is the GPU you want to test on\n  #: DATASET in {ade, coco, vg} is the dataset to train/test on, defined in the script\n  #: NET in {res50, res101} is the backbone networks to choose from\n  #: STEPS (x10K) is the number of iterations before it reduces learning rate, can support multiple steps separated by character 'a'\n  #: ITER (x10K) is the total number of iterations to run\n  #: Examples:\n  #: train on ADE20K for 320K iterations, reducing learning rate at 280K.\n  ./experiments/scripts/train.sh 0 ade 28 32\n  #: train on COCO for 720K iterations, reducing at 320K and 560K.\n  ./experiments/scripts/train.sh 1 coco 32a56 72\n  ```\n\n6. To train and test the reasoning modules (based on ResNet-50):\n  ```Shell\n  ./experiments/scripts/train_memory.sh [GPU_ID] [DATASET] [MEM] [STEPS] [ITER] \n  #: MEM in {local} is the type of reasoning modules to use \n  #: Examples:\n  #: train on ADE20K on the local spatial memory.\n  ./experiments/scripts/train_memory.sh 0 ade local 28 32\n  ```\n\n7. Once the training is done, you can test the models separately with `test.sh` and `test_memory.sh`, we also provided a separate set of scripts to test on larger image inputs.\n\n8. You can use tensorboard to visualize and track the progress, for example:\n  ```Shell\n  tensorboard --logdir=tensorboard/res50/ade_train_5/ --port=7002 &\n  ```\n\n", "correct": true}}, "DeepGuidedFilter-README.md": {"description": {"excerpt": "![](images/results.jpg)\n\n**DeepGuidedFilter** is the author's implementation of the deep learning building block for joint upsampling described in:  \n\n**Fast End-to-End Trainable Guided Filter**     \nHuikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang    \nCVPR 2018\n\nGiven a reference image pair in high-resolution and low-resolution, our algorithm generates high-resolution target from the low-resolution input. Through joint training with CNNs, our algorithm achieves the state-of-the-art performance while runs **10-100** times faster. \n\nContact: Hui-Kai Wu (huikaiwu@icloud.com)\n\n", "correct": true}, "citation": {"excerpt": "```\n@inproceedings{wu2017fast,\n  title     = {Fast End-to-End Trainable Guided Filter},\n  author    = {Wu, Huikai and Zheng, Shuai and Zhang, Junge and Huang, Kaiqi},\n  booktitle = {CVPR},\n  year = {2018}\n}\n```", "correct": true}, "installation": {"excerpt": "```sh\ngit checkout master\n\nconda install opencv\nconda install pytorch=0.2.0 cuda80 -c soumith\n    \npip install -r requirements.txt\n\n#: (Optional) For MonoDepth (TF Version).\npip install -r ComputerVision/MonoDepth/requirements.txt \n```\n", "correct": true}, "usage": {"excerpt": "* PyTorch Version\n    ```python\n    from guided_filter_pytorch.guided_filter import FastGuidedFilter\n    \n    hr_y = FastGuidedFilter(r, eps)(lr_x, lr_y, hr_x)\n    ```\n    ```python\n    from guided_filter_pytorch.guided_filter import GuidedFilter\n    \n    hr_y = GuidedFilter(r, eps)(hr_x, init_hr_y)\n    ``` \n* Tensorflow Version\n    ```python\n    from guided_filter_tf.guided_filter import fast_guided_filter\n    \n    hr_y = fast_guided_filter(lr_x, lr_y, hr_x, r, eps, nhwc)\n    ```\n    ```python\n    from guided_filter_tf.guided_filter import guided_filter\n    \n    hr_y = guided_filter(hr_x, init_hr_y, r, eps, nhwc)\n    ```\n", "correct": false}}, "gprMax-README.md": {"citation": {"excerpt": "If you use gprMax and publish your work we would be grateful if you could cite our work using:\n\n* Warren, C., Giannopoulos, A., & Giannakis I. (2016). gprMax: Open source software to simulate electromagnetic wave propagation for Ground Penetrating Radar, `Computer Physics Communications` (http://dx.doi.org/10.1016/j.cpc.2016.08.020)\n\nFor further information on referencing gprMax visit the `Publications section of our website `_.\n\n\n", "correct": true}, "installation": {"excerpt": "Once you have installed the aforementioned tools follow these steps to build and install gprMax:\n\n* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:\n\n.. code-block:: bash\n\n    (gprMax)$ python setup.py build\n    (gprMax)$ python setup.py install\n\n**You are now ready to proceed to running gprMax.**\n\nIf you have problems with building gprMax on Microsoft Windows, you may need to add :code:`C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin` to your path environment variable.\n\n", "correct": true}, "run": {"excerpt": "gprMax is designed as a Python package, i.e. a namespace which can contain multiple packages and modules, much like a directory.\n\nOpen a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`.\n\nBasic usage of gprMax is:\n\n.. code-block:: bash\n\n    (gprMax)$ python -m gprMax path_to/name_of_input_file\n\nFor example to run one of the test models:\n\n.. code-block:: bash\n\n    (gprMax)$ python -m gprMax user_models/cylinder_Ascan_2D.in\n\nWhen the simulation is complete you can plot the A-scan using:\n\n.. code-block:: bash\n\n    (gprMax)$ python -m tools.plot_Ascan user_models/cylinder_Ascan_2D.out\n\nYour results should like those from the A-scan from the metal cylinder example in `introductory/basic 2D models section `_\n\nWhen you are finished using gprMax, the conda environment can be deactivated using :code:`conda deactivate`.\n\n", "correct": true}, "support": {"excerpt": "Linux\n^^^^^\n\n* `gcc `_ should be already installed, so no action is required.\n\n\nmacOS\n^^^^^\n\n* Xcode (the IDE for macOS) comes with the LLVM (clang) compiler, but it does not currently support OpenMP, so you must install `gcc `_. That said, it is still useful to have Xcode (with command line tools) installed. It can be downloaded from the App Store. Once Xcode is installed, download and install the `Homebrew package manager `_ and then to install gcc, run:\n\n.. code-block:: bash\n\n    $ brew install gcc\n\nMicrosoft Windows\n^^^^^^^^^^^^^^^^^\n\n* Download and install `Microsoft Visual C++ 2015 Build Tools `_ (currently you must use the 2015 version, not 2017). Use the custom installation option and deselect everything apart from the Windows SDK for your version of Windows.\n\nAlternatively if you are using Windows 10 and feeling adventurous you can install the `Windows Subsystem for Linux `_ and then follow the Linux install instructions for gprMax. Note however that currently WSL does not aim to support GUI desktops or applications, e.g. Gnome, KDE, etc....\n\n\n\n", "correct": false}, "update": {"excerpt": "* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:\n\n.. code-block:: bash\n\n    (gprMax)$ git pull\n    (gprMax)$ python setup.py cleanall\n    (gprMax)$ python setup.py build\n    (gprMax)$ python setup.py install\n\nThis will pull the most recent gprMax source code form GitHub, remove/clean previously built modules, and then build and install the latest version of gprMax.\n\n\n", "correct": true}, "usage": {"excerpt": "We recommend using Miniconda to install Python and the required Python packages for gprMax in a self-contained Python environment. Miniconda is a mini version of Anaconda which is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.\n\n* `Download and install Miniconda `_. Choose the Python 3.x version for your platform. We recommend choosing the installation options to: install Miniconda only for your user account; add Miniconda to your PATH environment variable; and to register Miniconda Python as your default Python. See the `Quick Install page `_ for help installing Miniconda.\n* Open a Terminal (Linux/macOS) or Command Prompt (Windows) and run the following commands:\n\n.. code-block:: bash\n\n    $ conda update conda\n    $ conda install git\n    $ git clone https://github.com/gprMax/gprMax.git\n    $ cd gprMax\n    $ conda env create -f conda_env.yml\n\nThis will make sure conda is up-to-date, install Git, get the latest gprMax source code from GitHub, and create an environment for gprMax with all the necessary Python packages.\n\nIf you prefer to install Python and the required Python packages manually, i.e. without using Anaconda/Miniconda, look in the ``conda_env.yml`` file for a list of the requirements.\n\n", "correct": false}}, "gitfolio-README.md": {"installation": {"excerpt": "Install gitfolio\n\n```sh\nnpm i gitfolio -g\n```\n\n", "correct": true}, "license": {"excerpt": "![GitHub](https://img.shields.io/github/license/imfunniee/gitfolio.svg?style=popout-square)\n", "correct": true}, "update": {"excerpt": "To update your info, simply run\n\n```sh\n$ gitfolio update\n```\nThis will update your info and your repository info.\n\nTo Update background or theme you need to run `build` command again.\n\n\n", "correct": true}}, "gitbucket-gitbucket-README.md": {}, "neural_renderer-README.md": {"citation": {"excerpt": "```\n@InProceedings{kato2018renderer\n    title={Neural 3D Mesh Renderer},\n    author={Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},\n    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year={2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "```\nsudo python setup.py install\n```\n\n", "correct": false}, "run": {"excerpt": "```\npython ./examples/example1.py\npython ./examples/example2.py\npython ./examples/example3.py\npython ./examples/example4.py\n```\n\n\n", "correct": true}, "usage": {"excerpt": "```\npython ./examples/example1.py\npython ./examples/example2.py\npython ./examples/example3.py\npython ./examples/example4.py\n```\n\n\n", "correct": false}}, "facebookresearch-DensePose-README.md": {"citation": {"excerpt": "If you use Densepose, please use the following BibTeX entry.\n\n```\n  @InProceedings{Guler2018DensePose,\n  title={DensePose: Dense Human Pose Estimation In The Wild},\n  author={R\\{i}za Alp G\\\"uler, Natalia Neverova, Iasonas Kokkinos},\n  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2018}\n  }\n```\n\n\n", "correct": true}, "installation": {"excerpt": "Please find installation instructions for Caffe2 and DensePose in [`INSTALL.md`](INSTALL.md), a document based on the [Detectron](https://github.com/facebookresearch/Detectron) installation instructions.\n\n", "correct": true}, "license": {"excerpt": "This source code is licensed under the license found in the [`LICENSE`](LICENSE) file in the root directory of this source tree.\n\n", "correct": true}}, "node-qa-masker-README.md": {"installation": {"excerpt": "``` bash\nnpm install qa-masker\n```\n\n", "correct": true}}, "mapshaper-README.md": {"description": {"excerpt": "Mapshaper is software for editing Shapefile, GeoJSON, [TopoJSON](https://github.com/mbostock/topojson/wiki), CSV and several other data formats, written in JavaScript.\n\nThe `mapshaper` command line program supports essential map making tasks like simplifying shapes, editing attribute data, clipping, erasing, dissolving, filtering and more.\n\nThe web UI supports interactive simplification, attribute data editing, and running cli commands in a built-in console. Visit the public website at [www.mapshaper.org](http://www.mapshaper.org) or use the web UI locally via the `mapshaper-gui` script.\n\nSee the [project wiki](https://github.com/mbloch/mapshaper/wiki) for more documentation on how to use mapshaper.\n\nTo suggest improvements, add an [issue](https://github.com/mbloch/mapshaper/issues).\n\nTo learn about recent updates, read the [changelog](https://github.com/mbloch/mapshaper/releases).\n\n", "correct": true}, "license": {"excerpt": "This software is licensed under [MPL 2.0](http://www.mozilla.org/MPL/2.0/).\n\nAccording to Mozilla's [FAQ](http://www.mozilla.org/MPL/2.0/FAQ.html), \"The MPL's \u2018file-level\u2019 copyleft is designed to encourage contributors to share modifications they make to your code, while still allowing them to combine your code with code under other licenses (open or proprietary) with minimal restrictions.\"\n\n\n\n", "correct": true}, "run": {"excerpt": "bin/mapshaper-gui ", "correct": false}, "support": {"excerpt": "**Web interface**\n\nFirefox is able to load Shapefiles and GeoJSON files larger than 1GB. Chrome has improved in recent versions, but is still prone to out-of-memory errors when importing files larger than several hundred megabytes.\n\n**Command line interface**\n\nWhen working with very large files, mapshaper may become unresponsive or crash with the message \"JavaScript heap out of memory.\"\n\nOne option is to run `mapshaper-xl` (added in v0.4.63), which allocates more memory than the standard `mapshaper` program.\n\nAnother solution is to run Node directly with the `--max-old-space-size` option. The following example (Mac or Linux) allocates 8GB of memory:\n```bash\n$ node  --max-old-space-size=8192 `which mapshaper` \n```\n\n#:#:#: Installation\n\nMapshaper requires [Node.js](http://nodejs.org).\n\nWith Node installed, you can install the latest release version of mapshaper using npm. Install with the \"-g\" flag to make the executable scripts available systemwide.\n\n```bash\nnpm install -g mapshaper\n```\n\nTo install and run the latest development code from github:\n\n```bash\ngit clone git@github.com:mbloch/mapshaper.git\ncd mapshaper\nnpm install\nbin/mapshaper     ", "correct": false}, "usage": {"excerpt": "```\n\n", "correct": false}}, "empymod-README.md": {"license": {"excerpt": "Copyright 2016-2019 Dieter Werthm\u00fcller\n\nLicensed under the Apache License, Version 2.0. See the ``LICENSE``- and\n``NOTICE``-files or the documentation for more information.\n", "correct": true}}, "segyio-README.md": {"description": {"excerpt": "Opening a file for reading is done with the `segyio.open` function, and\nidiomatically used with context managers. Using the `with` statement, files are\nproperly closed even in the case of exceptions. By default, files are opened\nread-only.\n\n```python\nwith segyio.open(filename) as f:\n    ...\n```\n\nOpen accepts several options (for more a more comprehensive reference, check\nthe open function's docstring with `help(segyio.open)`. The most important\noption is the second (optional) positional argument. To open a file for\nwriting, do `segyio.open(filename, 'r+')`, from the C `fopen` function.\n\nFiles can be opened in *unstructured* mode, either by passing `segyio.open` the\noptional arguments `strict=False`, in which case not establishing structure\n(inline numbers, crossline numbers etc.) is not an error, and\n`ignore_geometry=True`, in which case segyio won't even try to set these\ninternal attributes.\n\nThe segy file object has several public attributes describing this structure:\n* `f.ilines`\n    Inferred inline numbers\n* `f.xlines`\n    Inferred crossline numbers\n* `f.offsets`\n    Inferred offsets numbers\n* `f.samples`\n    Inferred sample offsets (frequency and recording time delay)\n* `f.unstructured`\n    True if unstructured, False if structured\n* `f.ext_headers`\n    The number of extended textual headers\n\nIf the file is opened *unstructured*, all the line properties will will be\n`None`.\n\n", "correct": false}, "usage": {"excerpt": "When segyio is built and installed, you're ready to start programming! Check\nout the [tutorial](#tutorial), [examples](#examples), [example\nprograms](python/examples), and [example\nnotebooks](https://github.com/equinor/segyio-notebooks). For a technical\nreference with examples and small recipes, [read the\ndocs](https://segyio.readthedocs.io/). API docs are also available with pydoc -\nstart your favourite Python interpreter and type `help(segyio)`, which should\nintegrate well with IDLE, pycharm and other Python tools.\n\n", "correct": true}}, "vid2vid-README.md": {"citation": {"excerpt": "We thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.\nThis code borrows heavily from [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) and [pix2pixHD](https://github.com/NVIDIA/pix2pixHD).\n", "correct": false}, "installation": {"excerpt": "- Install python libraries [dominate](https://github.com/Knio/dominate) and requests.\n```bash\npip install dominate requests\n```\n- If you plan to train with face datasets, please install dlib.\n```bash\npip install dlib\n```\n- If you plan to train with pose datasets, please install [DensePose](https://github.com/facebookresearch/DensePose) and/or [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\n- Clone this repo:\n```bash\ngit clone https://github.com/NVIDIA/vid2vid\ncd vid2vid\n```\n- Docker Image\nIf you have difficulty building the repo, a docker image can be found in the `docker` folder.\n\n", "correct": true}, "requirement": {"excerpt": "- Linux or macOS\n- Python 3\n- NVIDIA GPU + CUDA cuDNN\n- PyTorch 0.4\n\n\n", "correct": true}}, "PVGeo-README.md": {"issues": {"excerpt": "Please feel free to post features you would like to see from this package on the\n[**issues page**](https://github.com/OpenGeoVis/PVGeo/issues) as a feature\nrequest.\nIf you stumble across any bugs or crashes while using code distributed here,\nreport them in the issues section so we can promptly address it.\nFor other questions, join the [***PVGeo* community on Slack**](http://slack.pvgeo.org).\n\nInterested in contributing to PVGeo? Please see the [contributing guide](https://pvgeo.org/dev-guide/contributing.html)\n\n", "correct": true}, "support": {"excerpt": "Please feel free to post features you would like to see from this package on the\n[**issues page**](https://github.com/OpenGeoVis/PVGeo/issues) as a feature\nrequest.\nIf you stumble across any bugs or crashes while using code distributed here,\nreport them in the issues section so we can promptly address it.\nFor other questions, join the [***PVGeo* community on Slack**](http://slack.pvgeo.org).\n\nInterested in contributing to PVGeo? Please see the [contributing guide](https://pvgeo.org/dev-guide/contributing.html)\n\n", "correct": true}, "usage": {"excerpt": "To begin using the *PVGeo* Python package, create/activate your Python virtual\nenvironment (we highly recommend using anaconda) and install *PVGeo* through pip:\n\n```bash\npip install PVGeo\n```\n\nNow *PVGeo* is ready for use in your standard Python environment (2.7 or >=3.6)\nwith all dependencies installed! Go ahead and test your install:\n\n```bash\npython -c \"import PVGeo; print(PVGeo.__version__)\"\n```\n\nNote that Windows users must use Python >=3.6 when outside of ParaView.\nFurther insight can be found in the [**Getting Started Guide**](http://pvgeo.org/overview/getting-started.html).\n\n\n", "correct": true}}, "hyvr-README.md": {"installation": {"excerpt": "Installing Python\n^^^^^^^^^^^^^^^^^\n\n\nWindows\n\"\"\"\"\"\"\"\n\nIf you are using Windows, we recommend installing the `Anaconda distribution\n`_ of Python 3. This distribution has the\nmajority of dependencies that HyVR requires.\n\nIt is also a good idea to install the HyVR package into a `virtual environment\n`_. Do this by\nopening a command prompt window and typing the following::\n\n    conda create --name hyvr_env\n\nYou need to then activate this environment::\n\n    conda activate hyvr_env\n\t\n\nLinux\n\"\"\"\"\"\n\nDepending on your preferences you can either use the Anaconda/Miniconda\ndistribution of python, or the version of your package manager. If you choose\nthe former, follow the same steps as for Windows.\n\nIf you choose the latter, you probably already have Python 3 installed. If not,\nyou can install it using your package manager (e.g. ``apt`` on Ubuntu/Debian).\n\nIn any way we recommend using a virtual environment. Non-conda users can use\n`virtualenvwrapper `_ or\n`pipenv `_.\n\n\nInstalling HyVR\n^^^^^^^^^^^^^^^\n\nOnce you have activated your virtual environment, you can install HyVR from PyPI using ``pip``::\n\n    pip install hyvr\n\nThe version on PyPI should always be up to date. If it's not, you can also\ninstall HyVR from github::\n\n    git clone https://github.com/driftingtides/hyvr.git\n    pip install hyvr\n\nTo install from source you need a C compiler.\n\nInstallation from conda-forge will (hopefully) be coming soon.\n\n\n", "correct": true}}, "CU-Net-README.md": {"description": {"excerpt": "The follwoing figure gives an illustration of naive dense U-Net, stacked U-Nets and coupled U-Nets (CU-Net). The naive dense U-Net and stacked U-Nets have shortcut connections only inside each U-Net. In contrast, the coupled U-Nets also have connections for semantic blocks across U-Nets. The CU-Net is a hybrid of naive dense U-Net and stacked U-Net, integrating the merits of both dense connectivity, intermediate supervisions and multi-stage top-down and bottom-up refinement. The resulted CU-Net could save ~70% parameters of the previous stacked U-Nets but with comparable accuracy.\n\n\nIf we couple each U-Net pair in multiple U-Nets, the coupling connections would have quadratic growth with respect to the U-Net number. To make the model more parameter efficient, we propose the order-K coupling to trim off the long-distance coupling connections.\n\nFor simplicity, each dot represents one U-Net. The red and blue lines are the shortcut connections of inside semantic blocks and outside inputs. Order-0 connectivity (Top) strings U-Nets together only by their inputs and outputs, i.e. stacked U-Nets. Order-1 connectivity (Middle) has shortcut connections for adjacent U-Nets. Similarly, order-2 connectivity (Bottom) has shortcut connections for 3 nearby U-Nets.\n\n", "correct": false}, "citation": {"excerpt": "If you find this code useful in your research, please consider citing:\n\n```\n@inproceedings{tang2018quantized,\n  title={Quantized densely connected U-Nets for efficient landmark localization},\n  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Wu, Lingfei and Zhang, Shaoting and Metaxas, Dimitris},\n  booktitle={ECCV},\n  year={2018}\n}\n@inproceedings{tang2018cu,\n  title={CU-Net: Coupled U-Nets},\n  author={Tang, Zhiqiang and Peng, Xi and Geng, Shijie and Zhu, Yizhe and Metaxas, Dimitris},\n  booktitle={BMVC},\n  year={2018}\n}\n```\n\n", "correct": true}, "requirement": {"excerpt": "This package has the following requirements:\n\n* `Python 2.7`\n* `Pytorch v0.4.0` or `Pytorch v0.1.12`\n\nNote that the script name with string `prev-version` requires `Pytorch v0.1.12`.\n\n", "correct": true}}, "cltk-cltk-README.md": {"citation": {"excerpt": "Each major release of the CLTK is given a [DOI](http://en.wikipedia.org/wiki/Digital_object_identifier), a type of unique identity for digital documents. This DOI ought to be included in your citation, as it will allow researchers to reproduce your results should the CLTK's API or codebase change. To find the CLTK's current DOI, observe the blue `DOI` button in the repository's home on GitHub. To the end of your bibliographic entry, append `DOI ` plus the current identifier. You may also add version/release number, located in the `pypi` button at the project's GitHub repository homepage.\n\nThus, please cite core software as something like:\n```\nKyle P. Johnson et al.. (2014-2019). CLTK: The Classical Language Toolkit. DOI 10.5281/zenodo.\n```\n\nA style-neutral BibTeX entry would look like this:\n```\n@Misc{johnson2014,\nauthor = {Kyle P. Johnson et al.},\ntitle = {CLTK: The Classical Language Toolkit},\nhowpublished = {\\url{https://github.com/cltk/cltk}},\nnote = {{DOI} 10.5281/zenodo.},\nyear = {2014--2019},\n}\n```\n\n\n[Many contributors](https://github.com/cltk/cltk/blob/master/contributors.md) have made substantial contributions to the CLTK. For scholarship about particular code, it might be proper to cite these individuals as authors of the work under discussion.\n\n\n", "correct": true}, "installation": {"excerpt": "CLTK supports Python versions 3.6 and 3.7. The software only runs on POSIX\u2013compliant operating systems (Linux, Mac OS X, FreeBSD, etc.).\n\n``` bash\n$ pip install cltk\n```\n\nSee docs for [complete installation instructions](http://docs.cltk.org/en/latest/installation.html).\n\nThe [CLTK organization curates corpora](https://github.com/cltk) which can be downloaded directly or, better, [imported by the toolkit](http://docs.cltk.org/en/latest/importing_corpora.html).\n\n\n", "correct": true}, "documentation": {"excerpt": "The docs are at [docs.cltk.org](http://docs.cltk.org).\n\n\n", "correct": true}, "license": {"excerpt": "The CLTK is Copyright (c) 2014-2019 Kyle P. Johnson, under the MIT License. See [LICENSE](https://github.com/cltk/cltk/blob/master/LICENSE) for details.\n", "correct": true}, "usage": {"excerpt": "For interactive tutorials, in the form of Jupyter Notebooks, see .\n\n\n", "correct": false}}, "pysal-README.md": {"license": {"excerpt": "See the file \\\"LICENSE.txt\\\" for information on the history of this\nsoftware, terms & conditions for usage, and a DISCLAIMER OF ALL\nWARRANTIES.\n", "correct": true}, "usage": {"excerpt": "If you are interested in contributing to PySAL please see our\n[development guidelines](https://github.com/pysal/pysal/wiki).\n\n", "correct": false}}, "ICNet-README.md": {"description": {"excerpt": "Based on [PSPNet](https://github.com/hszhao/PSPNet), this repository is build for evaluation in ICNet. For installation, please follow the description in PSPNet repository (support CUDA 7.0/7.5 + cuDNN v4).\n\n", "correct": false}, "citation": {"excerpt": "If ICNet is useful for your research, please consider citing:\n\n    @article{zhao2017icnet,\n      author = {Hengshuang Zhao and\n                Xiaojuan Qi and\n                Xiaoyong Shen and\n                Jianping Shi and\n                Jiaya Jia},\n      title = {ICNet for Real-Time Semantic Segmentation on High-Resolution Images},\n      journal={arXiv preprint arXiv:1704.08545},\n      year = {2017}\n    }\n", "correct": true}, "usage": {"excerpt": "1. Clone the repository recursively:\n\n   ```shell\n   git clone --recursive https://github.com/hszhao/ICNet.git\n   ```\n\n2. Build Caffe and matcaffe:\n\n   ```shell\n   cd $ICNET_ROOT/PSPNet\n   cp Makefile.config.example Makefile.config\n   vim Makefile.config\n   make -j8 && make matcaffe\n   cd ..\n   ```\n\n3. Evaluation mIoU:\n\n   - Evaluation code is in folder 'evaluation'.\n   - Download trained models and put them in folder 'evaluation/model':\n     - icnet_cityscapes_train_30k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCRXpXMnVIbXdfaW8) \n\n       (31M, md5: c7038630c4b6c869afaaadd811bdb539; train on trainset for 30k)\n\n     - icnet_cityscapes_trainval_90k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCTFVpZWJINi1Iblk) \n\n       (31M, md5: 4f4dd9eecd465dd8de7e4cf88ba5d5d5; train on trainvalset for 90k)\n   - Modify the related paths in 'eval_all.m':\n     - Mainly variables 'data_root' and 'eval_list', and your image list for evaluation should be similar to that in folder 'evaluation/samplelist' if you use this evaluation code structure. \n\n   ```shell\n   cd evaluation\n   vim eval_all.m\n   ```\n\n   - Run the evaluation scripts:\n\n   ```\n   ./run.sh\n   ```\n\n4. Evaluation time:\n\n   - To get inference time as accurate as possible, it's suggested to make sure the GPU card with specified ID in script 'test_time.sh' is empty (without other processes executing)\n\n   - Run the evaluation scripts:\n\n   ```\n   ./test_time.sh\n   ```\n\n5. Results: \n\n   - Prediction results will show in folder 'evaluation/mc_result' and the expected scores are:\n     - ICNet train on trainset for 30K, evaluated on valset (mIoU/pAcc): 67.7/94.5\n     - ICNet train on trainvalset for 90K, evaluated on testset (mIoU): 69.5\n   - Log information of inference time will be in file 'time.log', approximately 33~36ms on TitanX.\n\n6. Demo video:\n\n   - Video processed by ICNet on cityscapes dataset:\n     - Alpha blending with value as 0.5: [Video](https://youtu.be/qWl9idsCuLQ)\n\n", "correct": true}}, "tetgen-README.md": {"usage": {"excerpt": "    cells = grid.cells.reshape(-1, 5)[:, 1:]\n    cell_center = grid.points[cells].mean(1)\n\n    ", "correct": false}}, "geojson-vt-README.md": {"installation": {"excerpt": "Install using NPM (`npm install geojson-vt`) or Yarn (`yarn add geojson-vt`), then:\n\n```js\n// import as a ES module\nimport geojsonvt from 'geojson-vt';\n\n// or require in Node / Browserify\nconst geojsonvt = require('geojson-vt');\n```\n\nOr use a browser build directly:\n\n```html\n\n```\n", "correct": true}, "usage": {"excerpt": "Here's **geojson-vt** action in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),\ndynamically loading a 100Mb US zip codes GeoJSON with 5.4 million points:\n\n![](https://cloud.githubusercontent.com/assets/25395/5360312/86028d8e-7f91-11e4-811f-87f24acb09ca.gif)\n\nThere's a convenient [debug page](http://mapbox.github.io/geojson-vt/debug/) to test out **geojson-vt** on different data.\nJust drag any GeoJSON on the page, watching the console.\n\n![](https://cloud.githubusercontent.com/assets/25395/5363235/41955c6e-7fa8-11e4-9575-a66ef54cb6d9.gif)\n\n", "correct": true}}, "scikit-learn-scikit-learn-README.md": {"support": {"excerpt": "Documentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): http://scikit-learn.org\n- HTML documentation (development version): http://scikit-learn.org/dev/\n- FAQ: http://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\n- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn\n- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``\n- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn\n- Website: http://scikit-learn.org\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn\n", "correct": true}}, "DaSiamRPN-README.md": {"description": {"excerpt": "**SiamRPN** formulates the task of visual tracking as a task of localization and identification simultaneously, initially described in an [CVPR2018 spotlight paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf). (Slides at [CVPR 2018 Spotlight](https://drive.google.com/open?id=1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq))\n\n**DaSiamRPN** improves the performances of SiamRPN by (1) introducing an effective sampling strategy to control the imbalanced sample distribution, (2) designing a novel distractor-aware module to perform incremental learning, (3) making a long-term tracking extension. [ECCV2018](https://arxiv.org/pdf/1808.06048.pdf). (Slides at [VOT-18 Real-time challenge winners talk](https://drive.google.com/open?id=1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr))\n\n\n  \n\n\n", "correct": true}, "citation": {"excerpt": "If you find **DaSiamRPN** and **SiamRPN** useful in your research, please consider citing:\n\n```\n@inproceedings{Zhu_2018_ECCV,\n  title={Distractor-aware Siamese Networks for Visual Object Tracking},\n  author={Zhu, Zheng and Wang, Qiang and Bo, Li and Wu, Wei and Yan, Junjie and Hu, Weiming},\n  booktitle={European Conference on Computer Vision},\n  year={2018}\n}\n\n@InProceedings{Li_2018_CVPR,\n  title = {High Performance Visual Tracking With Siamese Region Proposal Network},\n  author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "- install pytorch, numpy, opencv following the instructions in the `run_install.sh`. Please do **not** use conda to install.\n- you can alternatively modify `/PATH/TO/CODE/FOLDER/` in `tracker_SiamRPN.m` \n  If the tracker is ready, you will see the tracking results. (EAO: 0.3827)\n\n\n", "correct": true}, "license": {"excerpt": "Licensed under an MIT license.\n\n\n", "correct": true}, "requirement": {"excerpt": "CPU: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz\nGPU: NVIDIA GTX1060\n\n- python2.7\n- pytorch == 0.3.1\n- numpy\n- opencv\n\n\n", "correct": true}, "usage": {"excerpt": "  \n\n\n- To reproduce the reuslts on paper, the pretrained model can be downloaded from [Google Drive](https://drive.google.com/open?id=1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H): `SiamRPNOTB.model`. \n:zap: :zap: This model is the **fastest** (~200fps) Siamese Tracker with AUC of 0.655 on OTB2015. :zap: :zap: \n\n- You must download OTB2015 dataset (download [script](code/data/get_otb_data.sh)) at first.\n\nA simple test example.\n\n```\ncd code\npython demo.py\n```\n\nIf you want to test the performance on OTB2015, please using the follwing command.\n\n```\ncd code\npython test_otb.py\npython eval_otb.py OTB2015 \"Siam*\" 0 1\n```\n\n\n", "correct": true}}, "scikit-image-scikit-image-README.md": {"citation": {"excerpt": "If you find this project useful, please cite:\n\n> St\u00e9fan van der Walt, Johannes L. Sch\u00f6nberger, Juan Nunez-Iglesias,\n> Fran\u00e7ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle\n> Gouillart, Tony Yu, and the scikit-image contributors.\n> *scikit-image: Image processing in Python*. PeerJ 2:e453 (2014)\n> https://doi.org/10.7717/peerj.453\n", "correct": true}, "installation": {"excerpt": "Install dependencies using:\n\n```\npip install -r requirements.txt\n```\n\nThen, install scikit-image using:\n\n```\n$ pip install .\n```\n\nIf you plan to develop the package, you may run it directly from source:\n\n```\n$ pip install -e .  #: Do this once to add package to Python path\n```\n\nEvery time you modify Cython files, also run:\n\n```\n$ python setup.py build_ext -i  #: Build binary extensions\n```\n\n", "correct": true}, "license": {"excerpt": "Copyright (C) 2011, the scikit-image team\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n 1. Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n 2. Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in\n    the documentation and/or other materials provided with the\n    distribution.\n 3. Neither the name of skimage nor the names of its contributors may be\n    used to endorse or promote products derived from this software without\n    specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\nSTRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\nIN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n\n", "correct": true}}, "geonotebook-README.md": {"installation": {"excerpt": "When developing geonotebook, it is often helpful to install packages as a reference to the\nchecked out repository rather than copying them to the system `site-packages`.  A \"development\ninstall\" will allow you to make live changes to python or javascript without reinstalling the\npackage.\n```bash\n#: Install the geonotebook python package as \"editable\"\npip install -e .\n\n#: Install the notebook extension as a symlink\njupyter nbextension install --sys-prefix --symlink --py geonotebook\n\n#: Enable the extension\njupyter serverextension enable --sys-prefix --py geonotebook\njupyter nbextension enable --sys-prefix --py geonotebook\n\n#: Start the javascript builder\ncd js\nnpm run watch\n```\n\n", "correct": true}, "requirement": {"excerpt": "For default tile serving\n  + GDAL >= 2.1.0\n  + mapnik >= 3.1.0\n  + python-mapnik >= 0.1\n\n", "correct": true}, "run": {"excerpt": "```bash\ncd notebooks/\njupyter notebook\n```\n\n", "correct": true}, "usage": {"excerpt": "First provision the geoserver\n\n```\ncd devops/geoserver/\nvagrant up\n```\n\nSecond change the ```vis_server``` configuration to ```geoserver``` in the ```[default]``` section of your configuration. Then include a ```[geoserver]``` section with the pertinent configuration.  E.g.:\n\n```\n[default]\nvis_server=geoserver\n\n...\n\n[geoserver]\nusername = admin\npassword = geoserver\nurl = http://127.0.0.1:8080/geoserver\n```\n", "correct": true}}, "generator-arcgis-js-app-README.md": {"license": {"excerpt": "MIT\n", "correct": true}, "usage": {"excerpt": "`grunt` - default task, will output code to a `dist` folder with sourcemaps.\n\n`grunt dev` - will start a local server on at `http://localhost:8282/` and watch for changes. Uses livereload to refresh browser with each update.\n\n`http://localhost:8282/dist/` - application\n\n`http://localhost:8282/node_modules/intern/client.html?config=tests/intern` - test suites\n\n`grunt build` - build the application and output to a `release` folder.\n\n`grunt e2e` - runs all tests using local [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/).\n\n\n", "correct": true}}, "tilelive-mapnik-README.md": {"installation": {"excerpt": "    npm install tilelive-mapnik\n\nThough `tilelive` is not a dependency of `tilelive-mapnik` you will want to\ninstall it to actually make use of `tilelive-mapnik` through a reasonable\nAPI.\n\n\n", "correct": true}, "usage": {"excerpt": "```javascript\nvar tilelive = require('tilelive');\nrequire('tilelive-mapnik').registerProtocols(tilelive);\n\ntilelive.load('mapnik:///path/to/file.xml', function(err, source) {\n    if (err) throw err;\n\n    // Interface is in XYZ/Google coordinates.\n    // Use `y = (1 << z) - 1 - y` to flip TMS coordinates.\n    source.getTile(0, 0, 0, function(err, tile, headers) {\n        // `err` is an error object when generation failed, otherwise null.\n        // `tile` contains the compressed image file as a Buffer\n        // `headers` is a hash with HTTP headers for the image.\n    });\n\n    // The `.getGrid` is implemented accordingly.\n});\n```\n\nNote that grid generation will only work when there's metadata inside a\n`` object in the Mapnik XML.\n\nThe key fields are `interactivity_layer` and `interactivity_fields`. See an\n[example in the tests](https://github.com/mapbox/tilelive-mapnik/blob/4e9cbf8347eba7c3c2b7e8fd4270ea39f9cc7af5/test/data/test.xml#L6-L7). These `Parameters` are normally added by the application that creates the XML,\nin this case [CartoCSS](https://github.com/mapbox/carto/blob/55fbafe0d0e8ec00515c5782a3664c15502f0437/lib/carto/renderer.js#L152-L189)\n", "correct": true}}, "map-vectorizer-README.md": {"requirement": {"excerpt": "A few things to be installed in your system in order to work properly. So far it has been **tested on Mac OS X Lion** so these instructions apply to that configuration only. I am sure you will be able to adapt it to your current configuration.\n\n* [Python] with [OpenCV] and [PIL] \n    * If you use [PIP](https://pypi.python.org/pypi) (recommended) you will get the necessary Python packages with: `pip install -r requirements.txt`\n* [R] - Make sure it is in your PATH (so you can run it via command-line by typing `R`).\n* You'll need the following R packages. On OS X simply navigate to `Packages & Data`, choose your CRAN mirror region, then search for and install:\n    * `alphahull` (you will need `tripack`, `sgeostat`, `splancs` as dependencies)\n    * `igraph`\n    * `shapefiles`\n    * `rgdal` (download the [binary for your OS](http://cran.r-project.org/web/packages/rgdal/index.html) then run `R CMD INSTALL --configure-args=\"\" path/to/rgdal.tar.gz`)\n    * You can also install the requirements by running this in the R CLI (by typing `R` in a terminal window):\n\n```\n    install.packages('rgdal')\n    install.packages('alphahull')\n    install.packages('igraph')\n    install.packages('shapefiles')\n```\n\n* Test that everything in R is installed, on the CLI you should be able to run this with no errors:\n\n```\n    library(rgdal)\n    library(alphahull)\n    library(igraph)\n    library(shapefiles)\n    quit() #: this will quit R\n```\n\n* [GIMP]\n* [GDAL Tools], on OS X try [version 1.9](http://www.kyngchaos.com/files/software/frameworks/GDAL_Complete-1.9.dmg). Per [MapBox](https://www.mapbox.com/tilemill/docs/guides/gdal/): The first time you install the GDAL package there is one additional step to make sure you can access these programs. In Mac OS, Open the Terminal application and run the following commands:\n\n```\n    echo 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH' >> ~/.bash_profile\n    source ~/.bash_profile\n```\n\n* It is also a good idea to install [QGIS] to test your results\n\n", "correct": true}, "run": {"excerpt": "These step by step instructions should work as-is. If not, **check all the above are working** before submitting an issue.\n\n1. Take note of the path where the GIMP executable is installed (the default value in the vectorizer is the Mac OS location: `/Applications/Gimp.app/Contents/MacOS/gimp-2.8`).\n2. Run the script on the provided test GeoTIFF:\n`python vectorize_map.py test.tif`\n3. Accept the GIMP folder location or input a different one and press ENTER.\n\n**NOTE:** The vectorizer has problems with *filenames that contain spaces*. This will be supported eventually.\n\nThis should take about 70 seconds to process. **If it takes less there might be an error** (or your machine rulez). Take a look at the console output to find the possible culprit.\n\nIf it works, you will see a `test` folder with a `test-traced` set of files (`.shp`, `.dbf`, `.prj` and `.shx`) and two log files.\n\n", "correct": true}, "usage": {"excerpt": "![Example input map](https://raw.github.com/NYPL/map-vectorizer/master/example_input.png)\n\n", "correct": true}}, "LapSRN-README.md": {"description": {"excerpt": "The Laplacian Pyramid Super-Resolution Network (LapSRN) is a progressive super-resolution model that super-resolves an low-resolution images in a coarse-to-fine Laplacian pyramid framework.\nOur method is fast and achieves state-of-the-art performance on five benchmark datasets for 4x and 8x SR.\nFor more details and evaluation results, please check out our [project webpage](http://vllab.ucmerced.edu/wlai24/LapSRN/) and [paper](http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf).\n\n![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)\n\n\n\n", "correct": true}, "citation": {"excerpt": "If you find the code and datasets useful in your research, please cite:\n    \n    @inproceedings{LapSRN,\n        author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, \n        title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, \n        booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},\n        year      = {2017}\n    }\n    \n\n", "correct": true}, "installation": {"excerpt": "Download repository:\n\n    $ git clone https://github.com/phoenix104104/LapSRN.git\n\nRun install.m in MATLAB to compile MatConvNet:\n\n    ", "correct": true}, "requirement": {"excerpt": "- MATLAB (we test with MATLAB R2017a on Ubuntu 16.04 and Windows 7)\n- Cuda & Cudnn (we test with Cuda 8.0 and Cudnn 5.1)\n\n", "correct": true}, "usage": {"excerpt": "    $ matlab\n    >> install\n   \nIf you install MatConvNet in your own path, you need to change the corresponding path in `install.m`, `train_LapSRN.m` and `test_LapSRN.m`.\n\n", "correct": true}}, "RESCAN-README.md": {"citation": {"excerpt": "If you use our code, please refer this repo.\nIf you publish your paper that refer to our paper, please cite:\n\n    @inproceedings{li2018recurrent,  \n        title={Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},  \n        author={Li, Xia and Wu, Jianlong and Lin, Zhouchen and Liu, Hong and Zha, Hongbin},  \n        booktitle={European Conference on Computer Vision},  \n        pages={262--277},  \n        year={2018},  \n        organization={Springer}  \n    }\n\n\n  [2]: http://cis.pku.edu.cn/faculty/vision/zlin/zlin.htm\n  [3]: http://robotics.pkusz.edu.cn/team/leader/\n  [4]: http://cis.pku.edu.cn/vision/Visual&Robot/people/zha/\n  [5]: ethanlee@pku.edu.cn\n  [6]: jlwu1992@pku.edu.cn\n  [7]: zlin@pku.edu.cn\n  [8]: hongliu@pku.edu.cn\n  [9]: http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html\n  [10]: https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s\n  \n", "correct": true}, "requirement": {"excerpt": "- Python>=3.6\n- Pytorch>=4.1.0\n- Opencv>=3.1.0\n- tensorboardX\n\n", "correct": true}, "usage": {"excerpt": "    python train.py\n    python eval.py\n    python show.py\n\n", "correct": true}}, "sg2im-README.md": {"installation": {"excerpt": "All code was developed and tested on Ubuntu 16.04 with Python 3.5 and PyTorch 0.4.\n\nYou can setup a virtual environment to run the code like this:\n\n```bash\npython3 -m venv env               #: Create a virtual environment\nsource env/bin/activate           #: Activate virtual environment\npip install -r requirements.txt   #: Install dependencies\necho $PWD > env/lib/python3.5/site-packages/sg2im.pth  #: Add current directory to python path\n#: Work for a while ...\ndeactivate  #: Exit virtual environment\n```\n\n", "correct": true}, "run": {"excerpt": "You can use the script `scripts/run_model.py` to easily run any of the pretrained models on new scene graphs using a simple human-readable JSON format. For example you can replicate the sheep images above like this:\n\n```bash\npython scripts/run_model.py \\\n  --checkpoint sg2im-models/vg128.pt \\\n  --scene_graphs scene_graphs/figure_6_sheep.json \\\n  --output_dir outputs\n```\n\nThe generated images will be saved to the directory specified by the `--output_dir` flag. You can control whether the model runs on CPU or GPU using py passing the flag `--device cpu` or `--device gpu`.\n\nWe provide JSON files and pretrained models allowing you to recreate all images from Figures 5 and 6 from the paper.\n\n", "correct": true}}, "pymeshfix-README.md": {}, "neural-motifs-README.md": {"installation": {"excerpt": "0. Install python3.6 and pytorch 3. I recommend the [Anaconda distribution](https://repo.continuum.io/archive/). To install PyTorch if you haven't already, use\n ```conda install pytorch=0.3.0 torchvision=0.2.0 cuda90 -c pytorch```.\n \n1. Update the config file with the dataset paths. Specifically:\n    - Visual Genome (the VG_100K folder, image_data.json, VG-SGG.h5, and VG-SGG-dicts.json). See data/stanford_filtered/README.md for the steps I used to download these.\n    - You'll also need to fix your PYTHONPATH: ```export PYTHONPATH=/home/rowan/code/scene-graph``` \n\n2. Compile everything. run ```make``` in the main directory: this compiles the Bilinear Interpolation operation for the RoIs as well as the Highway LSTM.\n\n3. Pretrain VG detection. The old version involved pretraining COCO as well, but we got rid of that for simplicity. Run ./scripts/pretrain_detector.sh\nNote: You might have to modify the learning rate and batch size, particularly if you don't have 3 Titan X GPUs (which is what I used). [You can also download the pretrained detector checkpoint here.](https://drive.google.com/open?id=11zKRr2OF5oclFL47kjFYBOxScotQzArX)\n\n4. Train VG scene graph classification: run ./scripts/train_models_sgcls.sh 2 (will run on GPU 2). OR, download the MotifNet-cls checkpoint here: [Motifnet-SGCls/PredCls](https://drive.google.com/open?id=12qziGKYjFD3LAnoy4zDT3bcg5QLC0qN6).\n5. Refine for detection: run ./scripts/refine_for_detection.sh 2 or download the [Motifnet-SGDet](https://drive.google.com/open?id=1thd_5uSamJQaXAPVGVOUZGAOfGCYZYmb) checkpoint.\n6. Evaluate: Refer to the scripts ./scripts/eval_models_sg[cls/det].sh.\n\n", "correct": true}, "support": {"excerpt": "Feel free to open an issue if you encounter trouble getting it to work!\n", "correct": true}}, "Flow-Guided-Feature-Aggregation-README.md": {"description": {"excerpt": "**Flow-Guided Feature Aggregation (FGFA)** is initially described in an [ICCV 2017 paper](https://arxiv.org/abs/1703.10025). It provides an accurate and end-to-end learning framework for video object detection. The proposed FGFA method, together with our previous work of [Deep Feature Flow](https://github.com/msracver/Deep-Feature-Flow), powered the winning entry of [ImageNet VID 2017](http://image-net.org/challenges/LSVRC/2017/results). It is worth noting that:\n\n* FGFA improves the per-frame features by aggregating nearby frame features along the motion paths. It significantly improves the object detection accuracy in videos, especially for fast moving objects.\n* FGFA is end-to-end trainable for the task of video object detection, which is vital for improving the recognition accuracy.\n* We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The [motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is included in this repository.\n\n***Click image to watch our demo video***\n\n[![Demo Video on YouTube](https://media.giphy.com/media/7D9tmDgzB10HK/giphy.gif)](https://www.youtube.com/watch?v=R2h3DbTPvVg)\n\n***Example object instances with slow, medium and fast motions***\n\n![Instance Motion](instance_motion.png)\n\n", "correct": true}, "citation": {"excerpt": "If you find Flow-Guided Feature Aggregation useful in your research, please consider citing:\n```\n@inproceedings{zhu17fgfa,\n    Author = {Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei},\n    Title = {Flow-Guided Feature Aggregation for Video Object Detection},\n    Conference = {ICCV},\n    Year = {2017}\n}\n\n@inproceedings{dai16rfcn,\n    Author = {Jifeng Dai, Yi Li, Kaiming He, Jian Sun},\n    Title = {{R-FCN}: Object Detection via Region-based Fully Convolutional Networks},\n    Conference = {NIPS},\n    Year = {2016}\n}\n```\n\n", "correct": true}, "installation": {"excerpt": "1. Please download ILSVRC2015 DET and ILSVRC2015 VID dataset, and make sure it looks like this:\n\n\t```\n\t./data/ILSVRC2015/\n\t./data/ILSVRC2015/Annotations/DET\n\t./data/ILSVRC2015/Annotations/VID\n\t./data/ILSVRC2015/Data/DET\n\t./data/ILSVRC2015/Data/VID\n\t./data/ILSVRC2015/ImageSets\n\t```\n\n2. Please download ImageNet pre-trained ResNet-v1-101 model and Flying-Chairs pre-trained FlowNet model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMOBdCBiNaKbcjPrA), and put it under folder `./model`. Make sure it looks like this:\n\t```\n\t./model/pretrained_model/resnet_v1_101-0000.params\n\t./model/pretrained_model/flownet-0000.params\n\t```\n\n", "correct": true}, "license": {"excerpt": "\u00a9 Microsoft, 2017. Licensed under the [MIT](LICENSE) License.\n\n", "correct": true}, "usage": {"excerpt": "1. To run the demo with our trained model (on ImageNet DET + VID train), please download the model manually from [OneDrive](https://1drv.ms/u/s!AqfHNsil2nOiiwDiKev7DB6L9ay7), and put it under folder `model/`.\n\n\tMake sure it looks like this:\n\t```\n\t./model/rfcn_fgfa_flownet_vid-0000.params\n\t```\n2. Run\n\t```\n\tpython ./fgfa_rfcn/demo.py\n\t```\n\n", "correct": true}}, "facebookresearch-pyrobot-README.md": {"citation": {"excerpt": "```\n@article{pyrobot2019,\n  title={PyRobot: An Open-source Robotics Framework for Research and Benchmarking},\n  author={Adithyavairavan Murali and Tao Chen and Kalyan Vasudev Alwala and Dhiraj Gandhi and Lerrel Pinto and Saurabh Gupta and Abhinav Gupta},\n  journal={arXiv preprint arXiv:1906.08236},\n  year={2019}\n}\n```\n", "correct": true}, "installation": {"excerpt": "* Install **Ubuntu 16.04** \n\n* Install [ROS kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu)\n\n* Install KDL\n\n```bash\nsudo apt-get -y install ros-kinetic-orocos-kdl ros-kinetic-kdl-parser-py ros-kinetic-python-orocos-kdl ros-kinetic-trac-ik\n```\n\n* Install Python virtual environment\n\n```bash\nsudo apt-get -y install python-virtualenv\nvirtualenv_name=\"pyenv_pyrobot\"\nVIRTUALENV_FOLDER=~/${virtualenv_name}\nvirtualenv --system-site-packages -p python2.7 $VIRTUALENV_FOLDER\n```\n\n* Install PyRobot \n\n```bash\ncd ~\nmkdir -p low_cost_ws/src\ncd ~/low_cost_ws/src\nsource ~/${virtualenv_name}/bin/activate\ngit clone --recurse-submodules https://github.com/facebookresearch/pyrobot.git\ncd pyrobot/\npip install .\n```\n\n**Warning**: As realsense keeps updating, compatibility issues might occur if you accidentally update \nrealsense-related packages from `Software Updater` in ubuntu. Therefore, we recommend you not to update\nany libraries related to realsense. Check the list of updates carefully when ubuntu prompts software udpates.\n\n", "correct": true}, "license": {"excerpt": "PyRobot is under MIT license, as found in the LICENSE file.\n", "correct": true}, "requirement": {"excerpt": "* Install **Ubuntu 16.04**\n\n* Download the installation script\n```bash\nsudo apt update\nsudo apt-get install curl\ncurl 'https://raw.githubusercontent.com/facebookresearch/pyrobot/master/robots/LoCoBot/install/locobot_install_all.sh' > locobot_install_all.sh\n```\n\n* Run the script to install everything (ROS, realsense driver, etc.). **Please connect the nuc machine to a realsense camera before running the following commands**.\n```bash\nchmod +x locobot_install_all.sh \n./locobot_install_all.sh\n```\n\n", "correct": true}, "usage": {"excerpt": "Please refer to [pyrobot.org](https://pyrobot.org/) and [locobot.org](http://locobot.org)\n\n", "correct": true}}, "Detectron-README.md": {"description": {"excerpt": "The goal of Detectron is to provide a high-quality, high-performance\ncodebase for object detection *research*. It is designed to be flexible in order\nto support rapid implementation and evaluation of novel research. Detectron\nincludes implementations of the following object detection algorithms:\n\n- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*\n- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*\n- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n- [RPN](https://arxiv.org/abs/1506.01497)\n- [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n- [R-FCN](https://arxiv.org/abs/1605.06409)\n\nusing the following backbone network architectures:\n\n- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)\n- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)\n- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)\n- [VGG16](https://arxiv.org/abs/1409.1556)\n\nAdditional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below.\n\n", "correct": true}, "citation": {"excerpt": "- [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440).\n  Ilija Radosavovic, Piotr Doll\u00e1r, Ross Girshick, Georgia Gkioxari, and Kaiming He.\n  Tech report, arXiv, Dec. 2017.\n- [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370).\n  Ronghang Hu, Piotr Doll\u00e1r, Kaiming He, Trevor Darrell, and Ross Girshick.\n  Tech report, arXiv, Nov. 2017.\n- [Non-Local Neural Networks](https://arxiv.org/abs/1711.07971).\n  Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\n  Tech report, arXiv, Nov. 2017.\n- [Mask R-CNN](https://arxiv.org/abs/1703.06870).\n  Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick.\n  IEEE International Conference on Computer Vision (ICCV), 2017.\n- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002).\n  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r.\n  IEEE International Conference on Computer Vision (ICCV), 2017.\n- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).\n  Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.\n  Tech report, arXiv, June 2017.\n- [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333).\n  Georgia Gkioxari, Ross Girshick, Piotr Doll\u00e1r, and Kaiming He.\n  Tech report, arXiv, Apr. 2017.\n- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144).\n  Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\n  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431).\n  Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He.\n  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n- [R-FCN: Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409).\n  Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.\n  Conference on Neural Information Processing Systems (NIPS), 2016.\n- [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)\n  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n  Conference on Neural Information Processing Systems (NIPS), 2015.\n- [Fast R-CNN](http://arxiv.org/abs/1504.08083).\n  Ross Girshick.\n  IEEE International Conference on Computer Vision (ICCV), 2015.\n", "correct": false}, "installation": {"excerpt": "Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).\n\n", "correct": true}, "license": {"excerpt": "Detectron is released under the [Apache 2.0 license](https://github.com/facebookresearch/detectron/blob/master/LICENSE). See the [NOTICE](https://github.com/facebookresearch/detectron/blob/master/NOTICE) file for additional details.\n\n", "correct": true}, "support": {"excerpt": "To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.\n\nIf bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.\n\n", "correct": true}, "update": {"excerpt": "- 4/2018: Support Group Normalization - see [`GN/README.md`](./projects/GN/README.md)\n\n", "correct": true}, "usage": {"excerpt": "To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.\n\nIf bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.\n\n", "correct": true}}, "DeepMVS-README.md": {"license": {"excerpt": "DeepMVS is licensed under the [BSD 2-Clause License](LICENSE.txt)", "correct": true}, "requirement": {"excerpt": "- **python 2.7**\n- **numpy 1.13.1**\n- **pytorch 0.3.0** and **torchvision**: Follow the instructions from [their website](http://pytorch.org/).\n- **opencv 3.1.0**: Run ``conda install -c menpo opencv`` or ``pip install opencv-python``.\n- **imageio 2.2.0** (with freeimage plugin): Run ``conda install -c conda-forge imageio`` or ``pip install imageio``. To install freeimage plugin, run the following Python script once:\n    ```python \n    import imageio\n    imageio.plugins.freeimage.download()\n    ```\n- **h5py 2.7.0**: Run ``conda install h5py`` or ``pip install h5py``.\n- **lz4 0.23.1**: Run ``pip install lz4``.\n- **cuda 8.0.61** and **16GB GPU RAM** (required for gpu support): The training codes use up to 14GB of the GPU RAM with the default configuration. We train our model with an NVIDIA Tesla P100 GPU. To reduce GPU RAM usage, feel free to try smaller ``--patch_width``, ``--patch_height``, ``--num_depths``, and ``--max_num_neighbors``. However, the resulting model may not show the efficacy as appeared in our paper.\n\n", "correct": true}}, "readgssi-README.md": {"citation": {"excerpt": "Ian M. Nesbitt, Fran\u00e7ois-Xavier Simon, Thomas Paulin, 2018. readgssi - an open-source tool to read and plot GSSI ground-penetrating radar data. [doi:10.5281/zenodo.1439119](https://dx.doi.org/10.5281/zenodo.1439119)\n\n", "correct": true}, "installation": {"excerpt": "If you choose to install a specific commit rather than the [latest working release of this software](https://pypi.org/project/readgssi), you may download this package, unzip to your home folder, open a command line, then install in the following way:\n\n```bash\npip install ~/readgssi\n```\n\n", "correct": false}, "contributor": {"excerpt": "- Ian Nesbitt ([@iannesbitt](https://github.com/iannesbitt), author)\n- Fran\u00e7ois-Xavier Simon ([@fxsimon](https://github.com/fxsimon))\n- Thomas Paulin ([@thomaspaulin](https://github.com/thomaspaulin))\n\n", "correct": true}, "requirement": {"excerpt": "Strongly recommended to install via [anaconda](https://www.anaconda.com/download):\n- [`obspy`](https://obspy.org/)\n- [`matplotlib`](https://matplotlib.org/)\n- [`numpy`](http://www.numpy.org/)\n- [`pandas`](https://pandas.pydata.org/)\n- [`h5py`](https://www.h5py.org/)\n\nInstall via `pip`:\n- [`pynmea2`](https://pypi.org/project/pynmea2/)\n- [`geopy`](https://pypi.org/project/geopy/)\n- [`pytz`](https://pypi.org/project/pytz/)\n\n", "correct": true}, "usage": {"excerpt": "To display the help text:\n\n```bash\n$ readgssi -h\n\nusage:\nreadgssi -i input.DZT [OPTIONS]\n\noptional flags:\n     OPTION     |      ARGUMENT       |       FUNCTIONALITY\n-o, --output    | file:  /dir/f.ext   |  specify an output file\n-f, --format    | string, eg. \"csv\"   |  specify output format (csv is the only working format currently)\n-p, --plot      | +integer or \"auto\"  |  plot will be x inches high (dpi=150), or \"auto\". default: 10\n-x, --xscale    | string, eg. \"dist\"  |  readgssi will attempt to convert the x-axis to distance, time, or traces based on header values\n-z, --zscale    | string, eg. \"time\"  |  readgssi will attempt to convert the x-axis to depth, time, or samples based on header values\n-n, --noshow    |                     |  suppress matplotlib popup window and simply save a figure (useful for multiple file processing)\n-c, --colormap  | string, eg. \"Greys\" |  specify the colormap (https://matplotlib.org/users/colormaps.html#:grayscale-conversion)\n-g, --gain      | positive (+)integer |  gain value (higher=greater contrast, default: 1)\n-r, --bgr       |                     |  horizontal background removal algorithm (useful to remove ringing)\n-R, --reverse   |                     |  reverse (flip radargram horizontally)\n-w, --dewow     |                     |  trinomial dewow algorithm\n-t, --bandpass  | +int-+int (MHz)     |  butterworth bandpass filter (positive integer range in megahertz; ex. 100-145)\n-b, --colorbar  |                     |  add a colorbar to the radar figure\n-a, --antfreq   | positive integer    |  specify antenna frequency (read automatically if not given)\n-s, --stack     | +integer or \"auto\"  |  specify trace stacking value or \"auto\" to autostack to ~2.5:1 x:y axis ratio\n-N, --normalize |                     |  reads a .DZG NMEA data if it exists; otherwise tries to read a csv file with lat, lon, and time fields to distance normalize with\n-d, --spm       | positive float      |  specify the samples per meter (spm) manually. overrides header value.\n-m, --histogram |                     |  produce a histogram of data values\n-E, --epsr      | float > 1.0         |  user-defined epsilon sub r (sometimes referred to as \"dielectric\"; ignores value in DZT header)\n-Z, --zero      | positive integer    |  skip this many samples from the top of the trace downward (useful for removing transceiver delay)\n\nnaming scheme for exports:\n   CHARACTERS   |      MEANING\n    c0          |  Profile from channel 0 (can range from 0 - 3)\n    Dn          |  Distance normalization\n    Tz233       |  Time zero at 233 samples\n    S8          |  Stacked 8 times\n    Rv          |  Profile read in reverse (flipped horizontally)\n    Bgr         |  Background removal filter\n    Dw          |  Dewow filter\n    Bp100-145   |  2-corner bandpass filter applied from 100 to 145 MHz\n    G30         |  30x contrast gain\n```\n\nFrom a unix command line:\n```bash\nreadgssi -i DZT__001.DZT\n```\nSimply specifying an input DZT file like in the above command (`-i file`) will display a host of data about the file including:\n- name of GSSI control unit\n- antenna model\n- antenna frequency\n- samples per trace\n- bits per sample\n- traces per second\n- L1 dielectric as entered during survey\n- sampling depth\n- speed of light at given dielectric\n- number of traces\n- number of seconds\n\n", "correct": true}}, "integral-human-pose-README.md": {"description": {"excerpt": "**Integral Regression** is initially described in an [ECCV 2018 paper](https://arxiv.org/abs/1711.08229). ([Slides](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)).\n\nWe build a [3D pose estimation system](https://arxiv.org/abs/1809.06079) based mainly on the Integral Regression, placing second in the [ECCV2018 3D Human Pose Estimation Challenge](http://vision.imar.ro/human3.6m/ranking.php). Note that, the winner [Sarandi et al.](https://arxiv.org/pdf/1809.04987.pdf) also uses the Integral Regression (or soft-argmax) with a better [augmented 3D dataset](https://github.com/isarandi/synthetic-occlusion) in their method indicating the Integral Regression is the currently state-of-the-art 3D human pose estimation method.\n\nThe Integral Regression is also known as soft-argmax. Please refer to two contemporary works ([Luvizon et al.](https://arxiv.org/abs/1710.02322) and [Nibali et al.](https://arxiv.org/abs/1801.07372)) for a better comparision and more comprehensive understanding.\n\n\n\n\n\n", "correct": true}, "citation": {"excerpt": "If you find Integral Regression useful in your research, please consider citing:\n```\n@article{sun2017integral,\n  title={Integral human pose regression},\n  author={Sun, Xiao and Xiao, Bin and Liang, Shuang and Wei, Yichen},\n  journal={arXiv preprint arXiv:1711.08229},\n  year={2017}\n}\n```\n```\n@article{sun2018integral,\n  title={An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge},\n  author={Sun, Xiao and Li, Chuankang and Lin, Stephen},\n  journal={arXiv preprint arXiv:1809.06079},\n  year={2018}\n}\n```\n\n", "correct": true}, "installation": {"excerpt": "1. Download Human3.6M(ECCV18 Challenge) image from [Human3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) and our processed annotation from [Baidu Disk](https://pan.baidu.com/s/1Qg4dH8PBXm8SzApI-uu0GA) (code: kfsm) or [Google Drive](https://drive.google.com/file/d/1wZynXUq91yECVRTFV8Tetvo271BXzxwI/view?usp=sharing)\n2. Download MPII image from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)\n3. Download COCO2017 image from [COCO Dataset](http://cocodataset.org/#home)\n4. Download cache file from [Dropbox](https://www.dropbox.com/sh/uouev0a1ao84ofd/AADAjJUdr_Fm-eubk7c_s2JTa?dl=0)\n5. Organize data like this\n```\n${PROJECT_ROOT}\n `-- data\n     `-- coco\n        |-- images\n        |-- annotations\n        |-- COCO_train2017_cache\n     `-- mpii\n        |-- images\n        |-- annot\n        |-- mpii_train_cache\n        |-- mpii_valid_cache\n     `-- hm36\n        |-- images\n        |-- annot\n        |-- HM36_train_cache\n        |-- HM36_validmin_cache\n     `-- hm36_eccv_challenge\n        `-- Train\n            |-- IMG\n            |-- POSE\n        `-- Val\n            |-- IMG\n            |-- POSE\n        `-- Test\n            |-- IMG\n        |-- HM36_eccv_challenge_Train_cache\n        |-- HM36_eccv_challenge_Test_cache\n        |-- HM36_eccv_challenge_Val_cache\n```\n\n#:#: Usage\nWe have placed some example config files in *experiments* folder, and you can use them straight forward. Don't modify them unless you know exactly what it means.\n#:#:#: Train \nFor [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229), cd to *pytorch_projects/integral_human_pose* \n**Integral Regression**\n```bash\npython train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs32-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/  \n```\n**Direct Joint Regression**\n```bash\npython train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_dj_l1_adam_bs32-4gpus_x140-90-120/lr1e-3.yaml --dataroot=../../data/\n```\n\nFor [3D pose estimation system](https://arxiv.org/abs/1809.06079) of ECCV18 Challenge, cd to *pytorch_projects/hm36_challenge*\n```bash\npython train.py --cfg=experiments/hm36/resnet152v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs24-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/\n```\n\nBy default, logging and model will be saved to *log* and *output* folder respectively.\n\n#:#:#: Test\nTo run evaluation on CHALL_H80K Val dataset\n1. Download [model](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)\n2. Place it under $project_root/model/hm36_challenge\n3. cd to *$project_root/pytorch_projects/hm36_challenge*\n4. execute command below\n```bash\npython test.py --cfg experiments/hm36/resnet152v1_ft/d-mch_384x288_deconv256x3_min-int-l1_adam_bs12-4gpus/lr1e-4_x300-270-290.yaml --model=../../model/hm36_challenge/model_chall_train_152ft_384x288.pth.tar\n```\n", "correct": true}, "license": {"excerpt": "\u00a9 Microsoft, 2017. Licensed under an MIT license.\n\n\n", "correct": true}}, "gdal-docker-README.md": {"usage": {"excerpt": "Running the container without any arguments will by default output the GDAL\nversion string as well as the supported raster and vector formats:\n\n    docker run geodata/gdal\n\nThe following command will open a bash shell in an Ubuntu based environment\nwith GDAL available:\n\n    docker run -t -i geodata/gdal /bin/bash\n\nYou will most likely want to work with data on the host system from within the\ndocker container, in which case run the container with the -v option. Assuming\nyou have a raster called `test.tif` in your current working directory on your\nhost system, running the following command should invoke `gdalinfo` on\n`test.tif`:\n\n    docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif\n\nThis works because the current working directory is set to `/data` in the\ncontainer, and you have mapped the current working directory on your host to\n`/data`.\n\nNote that the image tagged `latest`, GDAL represents the latest code *at the\ntime the image was built*. If you want to include the most up-to-date commits\nthen you need to build the docker image yourself locally along these lines:\n\n    docker build -t geodata/gdal:local git://github.com/geo-data/gdal-docker/\n", "correct": true}}, "pyro-ppl-pyro-README.md": {"citation": {"excerpt": "If you use Pyro, please consider citing:\n```\n@article{bingham2018pyro,\n  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and\n            Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and\n            Horsfall, Paul and Goodman, Noah D.},\n  title = {{Pyro: Deep Universal Probabilistic Programming}},\n  journal = {arXiv preprint arXiv:1810.09538},\n  year = {2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "**Install using pip:**\n\nPyro supports Python 3.4+.\n\n```sh\npip install pyro-ppl\n```\n\n**Install from source:**\n```sh\ngit clone git@github.com:pyro-ppl/pyro.git\ncd pyro\ngit checkout master  #: master is pinned to the latest release\npip install .\n```\n\n**Install with extra packages:**\n\nTo install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:\n```sh\npip install pyro-ppl[extras] \n```\nMake sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.\n\n", "correct": true}, "run": {"excerpt": "Refer to the instructions [here](docker/README.md).\n\n", "correct": true}}, "3D-ResNets-PyTorch-README.md": {"description": {"excerpt": "This is the PyTorch code for the following papers:\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\",  \nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018.\n](http://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html)\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition\",  \nProceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.\n](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf)\n\nThis code includes training, fine-tuning and testing on Kinetics, ActivityNet, UCF-101, and HMDB-51.  \n**If you want to classify your videos or extract video features of them using our pretrained models,\nuse [this code](https://github.com/kenshohara/video-classification-3d-cnn-pytorch).**\n\n**The Torch (Lua) version of this code is available [here](https://github.com/kenshohara/3D-ResNets).**  \nNote that the Torch version only includes ResNet-18, 34, 50, 101, and 152.\n\n", "correct": true}, "citation": {"excerpt": "If you use this code or pre-trained models, please cite the following:\n\n```bibtex\n@inproceedings{hara3dcnns,\n  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},\n  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  pages={6546--6555},\n  year={2018},\n}\n```\n\n", "correct": true}, "requirement": {"excerpt": "* [PyTorch](http://pytorch.org/)\n\n```bash\nconda install pytorch torchvision cuda80 -c soumith\n```\n\n* FFmpeg, FFprobe\n\n```bash\nwget http://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz\ntar xvf ffmpeg-release-64bit-static.tar.xz\ncd ./ffmpeg-3.3.3-64bit-static/; sudo cp ffmpeg ffprobe /usr/local/bin;\n```\n\n* Python 3\n\n", "correct": false}, "run": {"excerpt": "Assume the structure of data directories is the following:\n\n```misc\n~/\n  data/\n    kinetics_videos/\n      jpg/\n        .../ (directories of class names)\n          .../ (directories of video names)\n            ... (jpg files)\n    results/\n      save_100.pth\n    kinetics.json\n```\n\nConfirm all options.\n\n```bash\npython main.lua -h\n```\n\nTrain ResNets-34 on the Kinetics dataset (400 classes) with 4 CPU threads (for data loading).  \nBatch size is 128.  \nSave models at every 5 epochs.\nAll GPUs is used for the training.\nIf you want a part of GPUs, use ```CUDA_VISIBLE_DEVICES=...```.\n\n```bash\npython main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \\\n--result_path results --dataset kinetics --model resnet \\\n--model_depth 34 --n_classes 400 --batch_size 128 --n_threads 4 --checkpoint 5\n```\n\nContinue Training from epoch 101. (~/data/results/save_100.pth is loaded.)\n\n```bash\npython main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \\\n--result_path results --dataset kinetics --resume_path results/save_100.pth \\\n--model_depth 34 --n_classes 400 --batch_size 128 --n_threads 4 --checkpoint 5\n```\n\nFine-tuning conv5_x and fc layers of a pretrained model (~/data/models/resnet-34-kinetics.pth) on UCF-101.\n\n```bash\npython main.py --root_path ~/data --video_path ucf101_videos/jpg --annotation_path ucf101_01.json \\\n--result_path results --dataset ucf101 --n_classes 400 --n_finetune_classes 101 \\\n--pretrain_path models/resnet-34-kinetics.pth --ft_begin_index 4 \\\n--model resnet --model_depth 34 --resnet_shortcut A --batch_size 128 --n_threads 4 --checkpoint 5\n```\n", "correct": true}, "update": {"excerpt": "Our paper \"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\" is accepted to CVPR2018!  \nWe update the paper information.\n\n", "correct": false}}, "mplstereonet-README.md": {"usage": {"excerpt": "In most cases, you'll want to ``import mplstereonet`` and then make an axes\nwith ``projection=\"stereonet\"`` (By default, this is an equal-area stereonet).\nAlternately, you can use ``mplstereonet.subplots``, which functions identically\nto ``matplotlib.pyplot.subplots``, but creates stereonet axes.\n\nAs an example::\n\n    import matplotlib.pyplot as plt\n    import mplstereonet\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='stereonet')\n\n    strike, dip = 315, 30\n    ax.plane(strike, dip, 'g-', linewidth=2)\n    ax.pole(strike, dip, 'g^', markersize=18)\n    ax.rake(strike, dip, -25)\n    ax.grid()\n\n    plt.show()\n\n.. image:: http://joferkington.github.com/mplstereonet/images/basic.png\n    :alt: A basic stereonet with a plane, pole to the plane, and rake along the plane\n    :align: center\n    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/basic.py\n    \nPlanes, lines, poles, and rakes can be plotted using axes methods (e.g.\n``ax.line(plunge, bearing)`` or ``ax.rake(strike, dip, rake_angle)``).\n\nAll planar measurements are expected to follow the right-hand-rule to indicate\ndip direction. As an example, 315/30S would be 135/30 following the right-hand\nrule.\n\n", "correct": true}}, "striplog-README.md": {"installation": {"excerpt": "    python setup.py sdist\n    pip install dist/striplog-0.6.1.tar.gz    ", "correct": true}}, "harismuneer-Ultimate-Facebook-Scraper-README.md": {"citation": {"excerpt": "[![DOI](https://zenodo.org/badge/145763277.svg)](https://zenodo.org/badge/latestdoi/145763277)\n\nIf you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.\n\n\n\n\n----------------------------------------------------------------------------------------------------------------------------------------\n\n", "correct": true}, "installation": {"excerpt": "You will need to install latest version of [Google Chrome](https://www.google.com/chrome/). Moreover, you need to install selenium module as well using\n\n```\npip install selenium\n```\n\nRun the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.\nThe tool uses latest version of [Chrome Web Driver](http://chromedriver.chromium.org/downloads). I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.\n\n", "correct": true}, "issues": {"excerpt": "[![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues)\n\nIf you face any issue, you can create a new issue in the Issues Tab and I will be glad to help you out.\n\n", "correct": true}, "license": {"excerpt": "[![MIT](https://img.shields.io/cocoapods/l/AFNetworking.svg?style=style&label=License&maxAge=2592000)](../master/LICENSE)\n\nCopyright (c) 2018-present, harismuneer, Hassaan-Elahi                                                        \n", "correct": true}, "run": {"excerpt": "There's a file named \"input.txt\". You can add as many profiles as you want in the following format with each link on a new line:\n\n```\nhttps://www.facebook.com/andrew.ng.96\nhttps://www.facebook.com/zuck\n```\n\nMake sure the link only contains the username or id number at the end and not any other stuff. Make sure its in the format mentioned above.\n\nNote: There are two modes to download Friends Profile Pics and the user's Photos: Large Size and Small Size. You can change the following variables. By default they are set to Small Sized Pics because its really quick while Large Size Mode takes time depending on the number of pictures to download\n\n```\n#: whether to download the full image or its thumbnail (small size)\n#: if small size is True then it will be very quick else if its False then it will open each photo to download it\n#: and it will take much more time\nfriends_small_size = True\nphotos_small_size = True\n```\n----------------------------------------------------------------------------------------------------------------------------------------\n\n", "correct": true}}, "nextflow-io-nextflow-README.md": {"description": {"excerpt": "Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows.\nIt supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch,\nGoogle Genomics Pipelines, and Kubernetes. Additionally, it provides support for manage your workflow dependencies\nthrough built-in support for Conda, Docker, Singularity, and Modules.\n\n## Contents\n- [Rationale](#rationale)\n- [Quick start](#quick-start)\n- [Documentation](#documentation)\n- [Tool Management](#tool-management)\n  - [Conda environments](#conda-environments)\n  - [Docker and Singularity](#containers)\n  - [Environment Modules](#environment-modules)\n- [HPC Schedulers](#hpc-schedulers)\n  - [SGE](#hpc-schedulers)\n  - [Univa Grid Engine](#hpc-schedulers)\n  - [LSF](#hpc-schedulers)\n  - [SLURM](#hpc-schedulers)\n  - [PBS/Torque](#hpc-schedulers)\n  - [HTCondor (experimental)](#hpc-schedulers)\n- [Cloud Support](#cloud-support)\n  - [AWS Batch](#cloud-support)\n  - [AWS EC2](#cloud-support)\n  - [Google Cloud](#cloud-support)\n  - [Google Genomics Pipelines](#cloud-support)\n  - [Kubernetes](#cloud-support)\n- [Community](#community)\n- [Build from source](#build-from-source)\n- [Contributing](#contributing)\n- [License](#license)\n- [Citations](#citations)\n- [Credits](#credits)\n\n\n", "correct": true}, "download": {"excerpt": "Nextflow does not require any installation procedure, just download the distribution package by copying and pasting\nthis command in your terminal:\n\n```\ncurl -fsSL https://get.nextflow.io | bash\n```\n\nIt creates the ``nextflow`` executable file in the current directory. You may want to move it to a folder accessible from your ``$PATH``.\n\n", "correct": true}, "support": {"excerpt": "*Nextflow* also supports running workflows across various clouds and cloud technologies. *Nextflow* can create AWS EC2 or Google GCE clusters and deploy your workflow. Managed solutions from both Amazon and Google are also supported through AWS Batch and Google Genomics Pipelines. Additionally, *Nextflow* can run workflows on either on-prem or managed cloud Kubernetes clusters. \n\nCurrently supported cloud platforms:\n  + [AWS Batch](https://www.nextflow.io/docs/latest/awscloud.html#aws-batch)\n  + [AWS EC2](https://www.nextflow.io/docs/latest/awscloud.html)\n  + [Google GCE](https://www.nextflow.io/docs/latest/google.html)\n  + [Google Genomics Pipelines](https://www.nextflow.io/docs/latest/google.html#google-pipelines)\n  + [Kubernetes](https://www.nextflow.io/docs/latest/kubernetes.html)\n\n\n\n", "correct": false}}, "GANimation-README.md": {"citation": {"excerpt": "If you use this code or ideas from the paper for your research, please cite our paper:\n```\n@inproceedings{pumarola2018ganimation,\n    title={GANimation: Anatomically-aware Facial Animation from a Single Image},\n    author={A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer},\n    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n    year={2018}\n}\n```\n", "correct": true}, "installation": {"excerpt": "The code requires a directory containing the following files:\n- `imgs/`: folder with all image\n- `aus_openface.pkl`: dictionary containing the images action units.\n- `train_ids.csv`: file containing the images names to be used to train.\n- `test_ids.csv`: file containing the images names to be used to test.\n\nAn example of this directory is shown in `sample_dataset/`.\n\nTo generate the `aus_openface.pkl` extract each image Action Units with [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units) and store each output in a csv file the same name as the image. Then run:\n```\npython data/prepare_au_annotations.py\n```\n\n", "correct": true}, "requirement": {"excerpt": "- Install PyTorch (version 0.3.1), Torch Vision and dependencies from http://pytorch.org\n- Install requirements.txt (```pip install -r requirements.txt```)\n\n", "correct": true}, "run": {"excerpt": "To train:\n```\nbash launch/run_train.sh\n```\nTo test:\n```\npython test --input_path path/to/img\n```\n\n", "correct": true}}, "two-stream-dyntex-synth-README.md": {"citation": {"excerpt": "```\n@inproceedings{tesfaldet2018,\n  author = {Matthew Tesfaldet and Marcus A. Brubaker and Konstantinos G. Derpanis},\n  title = {Two-Stream Convolutional Networks for Dynamic Texture Synthesis},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2018}\n}\n```\n\n", "correct": true}, "installation": {"excerpt": "1. Store the appearance-stream tfmodel in `./models`.\n2. Store the dynamics-stream tfmodel in `./models`. The filepath to this model is your `--dynamics_model` path.\n\n", "correct": false}, "license": {"excerpt": "Two-Stream Convolutional Networks for Dynamic Texture Synthesis\nCopyright (C) 2018  Matthew Tesfaldet\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\n\nFor questions, please contact Matthew Tesfaldet ([mtesfald@eecs.yorku.ca](mailto:mtesfald@eecs.yorku.ca)).\n", "correct": true}, "requirement": {"excerpt": "- Tensorflow 1.3 (or latest, although not tested)\n- Preferably a Titan X for synthesizing 12 frames\n- Appearance-stream [tfmodel](https://drive.google.com/open?id=19KkFi92oWLzuOWnGo6Zsqe-2CCXFAoXZ)\n- Dynamics-stream [tfmodel](https://drive.google.com/open?id=1DHnzoNO-iTgMUTbUOLrigEmpPHmn_mT1)\n- [Dynamic textures](https://drive.google.com/open?id=0B5T9jWfa9iDySWJHZnpNZ2dHWUk)\n- [Static textures](https://drive.google.com/open?id=11yMiPXiuYvLCyoLfQf_dEG6kuav8h6_3) (for dynamics style transfer)\n\n", "correct": true}, "usage": {"excerpt": "```\npython synthesize.py --type=dts --gpu=0 --runid=\"my_cool_fish\" --dynamics_target=data/dynamic_textures/fish --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel\n```\n\n", "correct": true}}, "ipyleaflet-README.md": {"installation": {"excerpt": "For a development installation (requires npm):\n\n```\n$ git clone https://github.com/jupyter-widgets/ipyleaflet.git\n$ cd ipyleaflet\n$ pip install -e .\n$ jupyter nbextension install --py --symlink --sys-prefix ipyleaflet\n$ jupyter nbextension enable --py --sys-prefix ipyleaflet\n$ jupyter labextension install js  #: If you are developing on JupyterLab\n```\n\nNote for developers:\n\n- the ``-e`` pip option allows one to modify the Python code in-place. Restart the kernel in order to see the changes.\n- the ``--symlink`` argument on Linux or OS X allows one to modify the JavaScript code in-place. This feature is not available with Windows.\n\n    For automatically building the JavaScript code every time there is a change, run the following command from the ``ipyleaflet/js/`` directory:\n\n    ```\n    $ npm run watch\n    ```\n\n    If you are on JupyterLab you also need to run the following in a separate terminal:\n\n    ```\n    $ jupyter lab --watch\n    ```\n\n    Every time a JavaScript build has terminated you need to refresh the Notebook page in order to load the JavaScript code again.\n\n", "correct": true}, "documentation": {"excerpt": "To get started with using `ipyleaflet`, check out the full documentation\n\nhttps://ipyleaflet.readthedocs.io/\n\n", "correct": true}, "license": {"excerpt": "We use a shared copyright model that enables all contributors to maintain the\ncopyright on their contributions.\n\nThis software is licensed under the BSD-3-Clause license. See the [LICENSE](LICENSE) file for details.\n\n", "correct": true}, "usage": {"excerpt": "**Selecting a basemap for a leaflet map:**\n\n![Basemap Screencast](basemap.gif)\n\n**Loading a geojson map:**\n\n![GeoJSON Screencast](geojson.gif)\n\n**Making use of leafletjs primitives:**\n\n![Primitives Screencast](primitives.gif)\n\n**Using the splitmap control:**\n\n![Splitmap Screencast](splitmap.gif)\n\n**Displaying velocity data on the top of a map:**\n\n![Velocity Screencast](velocity.gif)\n\n**Choropleth layer:**\n\n![Choropleth Screencast](choropleth.gif)\n\n", "correct": true}}, "rasterio-README.md": {"installation": {"excerpt": "    ", "correct": false}, "usage": {"excerpt": "    ", "correct": false}}, "puppeteer-README.md": {"installation": {"excerpt": "We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.\n\n", "correct": false}, "issues": {"excerpt": "We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.\n\n", "correct": true}, "run": {"excerpt": "We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.\n\n", "correct": false}, "usage": {"excerpt": "Note: Puppeteer requires at least Node v6.4.0, but the examples below use async/await which is only supported in Node v7.6.0 or greater.\n\nPuppeteer will be familiar to people using other browser testing frameworks. You create an instance\nof `Browser`, open pages, and then manipulate them with [Puppeteer's API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#).\n\n**Example** - navigating to https://example.com and saving a screenshot as *example.png*:\n\nSave file as **example.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://example.com');\n  await page.screenshot({path: 'example.png'});\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode example.js\n```\n\nPuppeteer sets an initial page size to 800px x 600px, which defines the screenshot size. The page size can be customized  with [`Page.setViewport()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagesetviewportviewport).\n\n**Example** - create a PDF.\n\nSave file as **hn.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});\n  await page.pdf({path: 'hn.pdf', format: 'A4'});\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode hn.js\n```\n\nSee [`Page.pdf()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagepdfoptions) for more information about creating pdfs.\n\n**Example** - evaluate script in the context of the page\n\nSave file as **get-dimensions.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://example.com');\n\n  // Get the \"viewport\" of the page, as reported by the page.\n  const dimensions = await page.evaluate(() => {\n    return {\n      width: document.documentElement.clientWidth,\n      height: document.documentElement.clientHeight,\n      deviceScaleFactor: window.devicePixelRatio\n    };\n  });\n\n  console.log('Dimensions:', dimensions);\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode get-dimensions.js\n```\n\nSee [`Page.evaluate()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pageevaluatepagefunction-args) for more information on `evaluate` and related methods like `evaluateOnNewDocument` and `exposeFunction`.\n\n\n\n\n", "correct": true}}, "tensorflow-README.md": {"installation": {"excerpt": "To install the current release for CPU-only:\n\n```\npip install tensorflow\n```\n\nUse the GPU package for CUDA-enabled GPU cards:\n\n```\npip install tensorflow-gpu\n```\n\n*See [Installing TensorFlow](https://www.tensorflow.org/install) for detailed\ninstructions, and how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n**Nightly pip packages** * We are pleased to announce that TensorFlow now offers\nnightly pip packages under the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on PyPi.\nSimply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean\nenvironment to install the nightly TensorFlow build. We support CPU and GPU\npackages on Linux, Mac, and Windows.\n\n", "correct": true}, "license": {"excerpt": "[Apache License 2.0](LICENSE)\n", "correct": true}}, "vue-README.md": {"description": {"excerpt": "Vue (pronounced `/vju\u02d0/`, like view) is a **progressive framework** for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.\n\n", "correct": true}, "documentation": {"excerpt": "To check out [live examples](https://vuejs.org/v2/examples/) and docs, visit [vuejs.org](https://vuejs.org).\n\n", "correct": true}, "issues": {"excerpt": "Please make sure to read the [Issue Reporting Checklist](https://github.com/vuejs/vue/blob/dev/.github/CONTRIBUTING.md#issue-reporting-guidelines) before opening an issue. Issues not conforming to the guidelines may be closed immediately.\n\n", "correct": true}, "license": {"excerpt": "[MIT](http://opensource.org/licenses/MIT)\n\nCopyright (c) 2013-present, Yuxi (Evan) You\n", "correct": true}}, "reduxjs-react-redux-README.md": {"installation": {"excerpt": "React Redux requires **React 16.8.3 or later.**\n\n```\nnpm install --save react-redux\n```\n\nThis assumes that you\u2019re using [npm](http://npmjs.com/) package manager \nwith a module bundler like [Webpack](https://webpack.js.org/) or \n[Browserify](http://browserify.org/) to consume [CommonJS \nmodules](https://webpack.js.org/api/module-methods/#commonjs).\n\nIf you don\u2019t yet use [npm](http://npmjs.com/) or a modern module bundler, and would rather prefer a single-file [UMD](https://github.com/umdjs/umd) build that makes `ReactRedux` available as a global object, you can grab a pre-built version from [cdnjs](https://cdnjs.com/libraries/react-redux). We *don\u2019t* recommend this approach for any serious application, as most of the libraries complementary to Redux are only available on [npm](http://npmjs.com/).\n\n", "correct": true}, "documentation": {"excerpt": "The React Redux docs are now published at **https://react-redux.js.org** .\n\nWe're currently expanding and rewriting our docs content - check back soon for more updates!\n\n", "correct": true}, "license": {"excerpt": "MIT\n", "correct": true}}, "pyansys-README.md": {"issues": {"excerpt": "    k += sparse.diags(np.random.random(k.shape[0])/1E20, shape=k.shape)\n\n    ", "correct": false}, "usage": {"excerpt": "    ", "correct": false}}, "kosmtik-README.md": {"installation": {"excerpt": "Note: Node.js versions are moving very fast, and kosmtik or its dependencies are\nhardly totally up to date with latest release. Ideally, you should run the LTS\nversion of Node.js. You can use a Node.js version manager (like\n[NVM](https://github.com/creationix/nvm)) to help.\n\n    npm -g install kosmtik\n\nThis might need root/Administrator rights. If you cannot install globally\nyou can also install locally with\n\n    npm install kosmtik\n\nThis will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`\nbelow with `node node_modules/kosmtik/index.js`.\n\nTo reinstall all plugins:\n\n    kosmtik plugins --reinstall\n\n", "correct": true}, "update": {"excerpt": "Note: Node.js versions are moving very fast, and kosmtik or its dependencies are\nhardly totally up to date with latest release. Ideally, you should run the LTS\nversion of Node.js. You can use a Node.js version manager (like\n[NVM](https://github.com/creationix/nvm)) to help.\n\n    npm -g install kosmtik\n\nThis might need root/Administrator rights. If you cannot install globally\nyou can also install locally with\n\n    npm install kosmtik\n\nThis will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`\nbelow with `node node_modules/kosmtik/index.js`.\n\nTo reinstall all plugins:\n\n    kosmtik plugins --reinstall\n\n", "correct": false}, "usage": {"excerpt": "To get command line help, run:\n\n    kosmtik -h\n\nTo run a Carto project (or `.yml`, `.yaml`):\n\n    kosmtik serve \n\nThen open your browser at http://127.0.0.1:6789/.\n\n\nYou may also want to install plugins. To see the list of available ones, type:\n\n    kosmtik plugins --available\n\nAnd then pick one and install it like this:\n\n    kosmtik plugins --install pluginname\n\nFor example:\n\n    kosmtik plugins --install kosmtik-map-compare [--install kosmtik-overlay\u2026]\n\n\n", "correct": true}}, "pyGeoPressure-README.md": {"installation": {"excerpt": "`pyGeoPressure` is on `PyPI`:\n\n```bash\npip install pygeopressure\n```\n\n", "correct": true}, "documentation": {"excerpt": "Read the documentaion for detailed explanations, tutorials and references:\nhttps://pygeopressure.readthedocs.io/en/latest/\n\n", "correct": true}, "license": {"excerpt": "The project is licensed under the MIT license, see the file [LICENSE]() for details.\n", "correct": true}, "support": {"excerpt": "If you find a bug, please report it at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) by opening a new issue with `bug` label.\n\n", "correct": true}}, "DBNet-README.md": {"description": {"excerpt": "This work is based on our [research paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html), which appears in CVPR 2018. We propose a large-scale dataset for driving behavior learning, namely, DBNet. You can also check our [dataset webpage](http://www.dbehavior.net/) for a deeper introduction.\n\nIn this repository, we release __demo code__ and __partial prepared data__ for training with only images, as well as leveraging feature maps or point clouds. The prepared data are accessible [here](https://drive.google.com/open?id=14RPdVTwBTuCTo0tFeYmL_SyN8fD0g6Hc). (__More demo models and scripts are released soon!__)\n\n", "correct": true}, "citation": {"excerpt": "If you find our work useful in your research, please consider citing:\n\n\t@InProceedings{DBNet2018,\n\t  author = {Yiping Chen and Jingkang Wang and Jonathan Li and Cewu Lu and Zhipeng Luo and HanXue and Cheng Wang},\n\t  title = {LiDAR-Video Driving Dataset: Learning Driving Policies Effectively},\n\t  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n\t  month = {June},\n\t  year = {2018}\n\t}\n\n", "correct": true}, "contributor": {"excerpt": "DBNet was developed by [MVIG](http://www.mvig.org/), Shanghai Jiao Tong University* and [SCSC](http://scsc.xmu.edu.cn/) Lab, Xiamen University* (*alphabetical order*).\n\n", "correct": true}, "license": {"excerpt": "Our code is released under Apache 2.0 License. The copyright of DBNet could be checked [here](http://www.dbehavior.net/contact.html).\n", "correct": true}, "requirement": {"excerpt": "* **Tensorflow 1.2.0**\n* Python 2.7\n* CUDA 8.0+ (For GPU)\n* Python Libraries: numpy, scipy and __laspy__\n\nThe code has been tested with Python 2.7, Tensorflow 1.2.0, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04. But it may work on more machines (directly or through mini-modification), pull-requests or test report are well welcomed.\n\n", "correct": true}}}